[
  {
    "title": "Title:A Survey on Evaluation of Large Language Models",
    "authors": "Authors:Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie",
    "abstract": " Abstract:  Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: this https URL.      ",
    "date": "Submitted on 6 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.03109",
    "pdf_link": "https://arxiv.org/pdf/2307.03109",
    "prompted": true
  },
  {
    "title": "Title:Style Over Substance: Evaluation Biases for Large Language Models",
    "authors": "Authors:Minghao Wu, Alham Fikri Aji",
    "abstract": " Abstract:  As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Conventionally, human evaluations are considered the gold standard in natural language generation. Recent advancements incorporate state-of-the-art LLMs as proxies for human judges in evaluation processes. Nonetheless, the extent to which humans and LLMs are capable evaluators remains uncertain. This study aims to investigate the behavior of both crowd-sourced human and LLM-based judges when comparing outputs from different models. To accomplish this, we curate a dataset comprising intentionally flawed machine-generated answers. Our findings indicate that despite the potentially greater danger posed by factual errors, answers with factual errors were still rated more favorably compared to answers that were too short or contained grammatical errors. This highlights a concerning bias in the evaluation process. To address this issue, we propose to independently evaluate machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. We instantiate this idea with the Elo rating system, resulting in the Multi-Elo Rating System. Empirical results from our study reveal that this proposed approach significantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy. However, notable improvement is not observed in crowd-sourced-based evaluations, suggesting the need for further investigation and refinement.      ",
    "date": "Submitted on 6 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.03025",
    "pdf_link": "https://arxiv.org/pdf/2307.03025",
    "prompted": true
  },
  {
    "title": "Title:Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation",
    "authors": "Authors:Le Xiao, Xiaolin Chen",
    "abstract": " Abstract:  News summary generation is an important task in the field of intelligence analysis, which can provide accurate and comprehensive information to help people better understand and respond to complex real-world events. However, traditional news summary generation methods face some challenges, which are limited by the model itself and the amount of training data, as well as the influence of text noise, making it difficult to generate reliable information accurately. In this paper, we propose a new paradigm for news summary generation using LLM with powerful natural language understanding and generative capabilities. We use LLM to extract multiple structured event patterns from the events contained in news paragraphs, evolve the event pattern population with genetic algorithm, and select the most adaptive event pattern to input into the LLM to generate news summaries. A News Summary Generator (NSG) is designed to select and evolve the event pattern populations and generate news summaries. The experimental results show that the news summary generator is able to generate accurate and reliable news summaries with some generalization ability.      ",
    "date": "Submitted on 6 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02839",
    "pdf_link": "https://arxiv.org/pdf/2307.02839",
    "prompted": true
  },
  {
    "title": "Title:PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
    "authors": "Authors:Ruosen Li, Teerth Patel, Xinya Du",
    "abstract": " Abstract:  Nowadays, the quality of responses generated by different modern large language models (LLMs) are hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs as a reference-free metric for open-ended question answering. More specifically, they use the recognized \"strongest\" LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on preferences of two answers. We conduct experiments on two benchmark datasets. We find that our approaches achieve higher accuracy and align better with human judgments, respectively. Interestingly, PR can induce a relatively accurate self-ranking of models under the anonymous setting, where each model's name is unrevealed. Our work provides space to explore evaluating models that are hard to compare for humans.      ",
    "date": "Submitted on 6 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02762",
    "pdf_link": "https://arxiv.org/pdf/2307.02762",
    "prompted": true
  },
  {
    "title": "Title:Improving Retrieval-Augmented Large Language Models via Data Importance Learning",
    "authors": "Authors:Xiaozhong Lyu, Stefan Grafberger, Samantha Biegel, Shaopeng Wei, Meng Cao, Sebastian Schelter, Ce Zhang",
    "abstract": " Abstract:  Retrieval augmentation enables large language models to take advantage of external knowledge, for example on tasks like question answering and data imputation. However, the performance of such retrieval-augmented models is limited by the data quality of their underlying retrieval corpus. In this paper, we propose an algorithm based on multilinear extension for evaluating the data importance of retrieved data points. There are exponentially many terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the data importance of data points in the retrieval corpus using the multilinear extension of the model's utility function. We further proposed an even more efficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental results illustrate that we can enhance the performance of large language models by only pruning or reweighting the retrieval corpus, without requiring further training. For some tasks, this even allows a small model (e.g., GPT-JT), augmented with a search engine API, to outperform GPT-3.5 (without retrieval augmentation). Moreover, we show that weights based on multilinear extension can be computed efficiently in practice (e.g., in less than ten minutes for a corpus with 100 million elements).      ",
    "date": "Submitted on 6 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.03027",
    "pdf_link": "https://arxiv.org/pdf/2307.03027",
    "prompted": true
  },
  {
    "title": "Title:What Should Data Science Education Do with Large Language Models?",
    "authors": "Authors:Xinming Tu, James Zou, Weijie J. Su, Linjun Zhang",
    "abstract": " Abstract:  The rapid advances of large language models (LLMs), such as ChatGPT, are revolutionizing data science and statistics. These state-of-the-art tools can streamline complex processes. As a result, it reshapes the role of data scientists. We argue that LLMs are transforming the responsibilities of data scientists, shifting their focus from hands-on coding, data-wrangling and conducting standard analyses to assessing and managing analyses performed by these automated AIs. This evolution of roles is reminiscent of the transition from a software engineer to a product manager. We illustrate this transition with concrete data science case studies using LLMs in this paper. These developments necessitate a meaningful evolution in data science education. Pedagogy must now place greater emphasis on cultivating diverse skillsets among students, such as LLM-informed creativity, critical thinking, AI-guided programming. LLMs can also play a significant role in the classroom as interactive teaching and learning tools, contributing to personalized education. This paper discusses the opportunities, resources and open challenges for each of these directions. As with any transformative technology, integrating LLMs into education calls for careful consideration. While LLMs can perform repetitive tasks efficiently, it's crucial to remember that their role is to supplement human intelligence and creativity, not to replace it. Therefore, the new era of data science education should balance the benefits of LLMs while fostering complementary human expertise and innovations. In conclusion, the rise of LLMs heralds a transformative period for data science and its education. This paper seeks to shed light on the emerging trends, potential opportunities, and challenges accompanying this paradigm shift, hoping to spark further discourse and investigation into this exciting, uncharted territory.      ",
    "date": "Submitted on 6 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02792",
    "pdf_link": "https://arxiv.org/pdf/2307.02792",
    "prompted": true
  },
  {
    "title": "Title:RecallM: An Architecture for Temporal Context Understanding and Question Answering",
    "authors": "Authors:Brandon Kynoch, Hugo Latapie",
    "abstract": " Abstract:  The ideal long-term memory mechanism for Large Language Model (LLM) based chatbots, would lay the foundation for continual learning, complex reasoning and allow sequential and temporal dependencies to be learnt. Creating this type of memory mechanism is an extremely challenging problem. In this paper we explore different methods of achieving the effect of long-term memory. We propose a new architecture focused on creating adaptable and updatable long-term memory for AGI systems. We demonstrate through various experiments the benefits of the RecallM architecture, particularly the improved temporal understanding it provides.      ",
    "date": "Submitted on 6 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02738",
    "pdf_link": "https://arxiv.org/pdf/2307.02738",
    "prompted": true
  },
  {
    "title": "Title:SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference",
    "authors": "Authors:Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, Subhabrata Mukherjee",
    "abstract": " Abstract:  Autoregressive large language models (LLMs) have made remarkable progress in various natural language generation tasks. However, they incur high computation cost and latency resulting from the autoregressive token-by-token generation. To address this issue, several approaches have been proposed to reduce computational cost using early-exit strategies. These strategies enable faster text generation using reduced computation without applying the full computation graph to each token. While existing token-level early exit methods show promising results for online inference, they cannot be readily applied for batch inferencing and Key-Value caching. This is because they have to wait until the last token in a batch exits before they can stop computing. This severely limits the practical application of such techniques. In this paper, we propose a simple and effective token-level early exit method, SkipDecode, designed to work seamlessly with batch inferencing and KV caching. It overcomes prior constraints by setting up a singular exit point for every token in a batch at each sequence position. It also guarantees a monotonic decrease in exit points, thereby eliminating the need to recompute KV Caches for preceding tokens. Rather than terminating computation prematurely as in prior works, our approach bypasses lower to middle layers, devoting most of the computational resources to upper layers, allowing later tokens to benefit from the compute expenditure by earlier tokens. Our experimental results show that SkipDecode can obtain 2x to 5x inference speedups with negligible regression across a variety of tasks. This is achieved using OPT models of 1.3 billion and 6.7 billion parameters, all the while being directly compatible with batching and KV caching optimization techniques.      ",
    "date": "Submitted on 5 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02628",
    "pdf_link": "https://arxiv.org/pdf/2307.02628",
    "prompted": true
  },
  {
    "title": "Title:Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard",
    "authors": "Authors:Xuan-Quy Dao",
    "abstract": " Abstract:  This paper presents a performance comparison of three large language models (LLMs), namely OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard, on the VNHSGE English dataset. The results show that BingChat is better than ChatGPT and Bard. Therefore, BingChat and Bard can replace ChatGPT while ChatGPT is not yet officially available in Vietnam. The results also indicate that ChatGPT, Bing Chat, and Bard outperform Vietnamese students in English language proficiency. The findings of this study contribute to the understanding of the potential of LLMs in English language education. The remarkable performance of ChatGPT, Bing Chat, and Bard demonstrates their potential as effective tools for teaching and learning English at the high school level.      ",
    "date": "Submitted on 5 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02288",
    "pdf_link": "https://arxiv.org/pdf/2307.02288",
    "prompted": true
  },
  {
    "title": "Title:Citation: A Key to Building Responsible and Accountable Large Language Models",
    "authors": "Authors:Jie Huang, Kevin Chen-Chuan Chang",
    "abstract": " Abstract:  Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify \"citation\" as a crucial yet missing component in LLMs, which could enhance content transparency and verifiability while addressing IP and ethical dilemmas. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the inherent potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable LLMs.      ",
    "date": "Submitted on 5 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02185",
    "pdf_link": "https://arxiv.org/pdf/2307.02185",
    "prompted": true
  },
  {
    "title": "Title:Open-Source Large Language Models Outperform Crowd Workers and Approach ChatGPT in Text-Annotation Tasks",
    "authors": "Authors:Meysam Alizadeh, Ma\u00ebl Kubli, Zeynab Samei, Shirin Dehghani, Juan Diego Bermeo, Maria Korobeynikova, Fabrizio Gilardi",
    "abstract": " Abstract:  This study examines the performance of open-source Large Language Models (LLMs) in text annotation tasks and compares it with proprietary models like ChatGPT and human-based services such as MTurk. While prior research demonstrated the high performance of ChatGPT across numerous NLP tasks, open-source LLMs like HugginChat and FLAN are gaining attention for their cost-effectiveness, transparency, reproducibility, and superior data protection. We assess these models using both zero-shot and few-shot approaches and different temperature parameters across a range of text annotation tasks. Our findings show that while ChatGPT achieves the best performance in most tasks, open-source LLMs not only outperform MTurk but also demonstrate competitive potential against ChatGPT in specific tasks.      ",
    "date": "Submitted on 5 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02179",
    "pdf_link": "https://arxiv.org/pdf/2307.02179",
    "prompted": true
  },
  {
    "title": "Title:PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records",
    "authors": "Authors:Viktor Schlegel, Hao Li, Yuping Wu, Anand Subramanian, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Daniel Beck, Xiaojun Zeng, Riza Theresa Batista-Navarro, Stefan Winkler, Goran Nenadic",
    "abstract": " Abstract:  This paper describes PULSAR, our system submission at the ImageClef 2023 MediQA-Sum task on summarising patient-doctor dialogues into clinical records. The proposed framework relies on domain-specific pre-training, to produce a specialised language model which is trained on task-specific natural data augmented by synthetic data generated by a black-box LLM. We find limited evidence towards the efficacy of domain-specific pre-training and data augmentation, while scaling up the language model yields the best performance gains. Our approach was ranked second and third among 13 submissions on task B of the challenge. Our code is available at this https URL.      ",
    "date": "Submitted on 5 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02006",
    "pdf_link": "https://arxiv.org/pdf/2307.02006",
    "prompted": true
  },
  {
    "title": "Title:Building Cooperative Embodied Agents Modularly with Large Language Models",
    "authors": "Authors:Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan",
    "abstract": " Abstract:  Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains. However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents. In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments. Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting. We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for embodied AI and lays the foundation for future research in multi-agent cooperation. Videos can be found on the project website this https URL.      ",
    "date": "Submitted on 5 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02485",
    "pdf_link": "https://arxiv.org/pdf/2307.02485",
    "prompted": true
  },
  {
    "title": "Title:Generative Job Recommendations with Large Language Model",
    "authors": "Authors:Zhi Zheng, Zhaopeng Qiu, Xiao Hu, Likang Wu, Hengshu Zhu, Hui Xiong",
    "abstract": " Abstract:  The rapid development of online recruitment services has encouraged the utilization of recommender systems to streamline the job seeking process. Predominantly, current job recommendations deploy either collaborative filtering or person-job matching strategies. However, these models tend to operate as \"black-box\" systems and lack the capacity to offer explainable guidance to job seekers. Moreover, conventional matching-based recommendation methods are limited to retrieving and ranking existing jobs in the database, restricting their potential as comprehensive career AI advisors. To this end, here we present GIRL (GeneratIve job Recommendation based on Large language models), a novel approach inspired by recent advancements in the field of Large Language Models (LLMs). We initially employ a Supervised Fine-Tuning (SFT) strategy to instruct the LLM-based generator in crafting suitable Job Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker. Moreover, we propose to train a model which can evaluate the matching degree between CVs and JDs as a reward model, and we use Proximal Policy Optimization (PPO)-based Reinforcement Learning (RL) method to further fine-tine the generator. This aligns the generator with recruiter feedback, tailoring the output to better meet employer preferences. In particular, GIRL serves as a job seeker-centric generative model, providing job suggestions without the need of a candidate set. This capability also enhances the performance of existing job recommendation models by supplementing job seeking features with generated content. With extensive experiments on a large-scale real-world dataset, we demonstrate the substantial effectiveness of our approach. We believe that GIRL introduces a paradigm-shifting approach to job recommendation systems, fostering a more personalized and comprehensive job-seeking experience.      ",
    "date": "Submitted on 5 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02157",
    "pdf_link": "https://arxiv.org/pdf/2307.02157",
    "prompted": true
  },
  {
    "title": "Title:Recommender Systems in the Era of Large Language Models (LLMs)",
    "authors": "Authors:Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, Qing Li",
    "abstract": " Abstract:  With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, recent studies have attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems, to provide researchers in relevant fields with an in-depth understanding. Therefore, in this paper, we conduct a comprehensive review of LLM-empowered recommender systems from various aspects including Pre-training, Fine-tuning, and Prompting. More specifically, we first introduce representative methods to harness the power of LLMs (as a feature encoder) for learning representations of users and items. Then, we review recent techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss future directions in this emerging field.      ",
    "date": "Submitted on 5 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02046",
    "pdf_link": "https://arxiv.org/pdf/2307.02046",
    "prompted": true
  },
  {
    "title": "Title:mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding",
    "authors": "Authors:Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang",
    "abstract": " Abstract:  Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy. We also build an OCR-free document instruction understanding evaluation set LLMDoc to better compare models' capabilities on instruct compliance and document understanding. Experimental results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOwl generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at this https URL.      ",
    "date": "Submitted on 4 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.02499",
    "pdf_link": "https://arxiv.org/pdf/2307.02499",
    "prompted": true
  },
  {
    "title": "Title:ProPILE: Probing Privacy Leakage in Large Language Models",
    "authors": "Authors:Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, Seong Joon Oh",
    "abstract": " Abstract:  The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.      ",
    "date": "Submitted on 4 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.01881",
    "pdf_link": "https://arxiv.org/pdf/2307.01881",
    "prompted": true
  },
  {
    "title": "Title:Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
    "authors": "Authors:Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, Kaidi Xu",
    "abstract": " Abstract:  Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular \"off-the-shelf\" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at this https URL.      ",
    "date": "Submitted on 3 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.01379",
    "pdf_link": "https://arxiv.org/pdf/2307.01379",
    "prompted": true
  },
  {
    "title": "Title:Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking",
    "authors": "Authors:Qinyong Wang, Zhenxiang Gao, Rong Xu",
    "abstract": " Abstract:  The biomedical field relies heavily on concept linking in various areas such as literature mining, graph alignment, information retrieval, question-answering, data, and knowledge integration. Although large language models (LLMs) have made significant strides in many natural language processing tasks, their effectiveness in biomedical concept mapping is yet to be fully explored. This research investigates a method that exploits the in-context learning (ICL) capabilities of large models for biomedical concept linking. The proposed approach adopts a two-stage retrieve-and-rank framework. Initially, biomedical concepts are embedded using language models, and then embedding similarity is utilized to retrieve the top candidates. These candidates' contextual information is subsequently incorporated into the prompt and processed by a large language model to re-rank the concepts. This approach achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7% in chemical entity normalization, exhibiting a competitive performance relative to supervised learning methods. Further, it showed a significant improvement, with an over 20-point absolute increase in F1 score on an oncology matching dataset. Extensive qualitative assessments were conducted, and the benefits and potential shortcomings of using large language models within the biomedical domain were discussed. were discussed.      ",
    "date": "Submitted on 3 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.01137",
    "pdf_link": "https://arxiv.org/pdf/2307.01137",
    "prompted": true
  },
  {
    "title": "Title:Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction",
    "authors": "Authors:Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pompianu, Sandro Gabriele Tiddia",
    "abstract": " Abstract:  In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for \"guiding\" the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.      ",
    "date": "Submitted on 3 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.01128",
    "pdf_link": "https://arxiv.org/pdf/2307.01128",
    "prompted": true
  },
  {
    "title": "Title:SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions",
    "authors": "Authors:Sameera Horawalavithana, Sai Munikoti, Ian Stewart, Henry Kvinge",
    "abstract": " Abstract:  Instruction finetuning is a popular paradigm to align large language models (LLM) with human intent. Despite its popularity, this idea is less explored in improving the LLMs to align existing foundation models with scientific disciplines, concepts and goals. In this work, we present SciTune as a tuning framework to improve the ability of LLMs to follow scientific multimodal instructions. To test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model LLaMA-SciTune that connects a vision encoder and LLM for science-focused visual and language understanding. In comparison to the models that are finetuned with machine generated data only, LLaMA-SciTune surpasses human performance on average and in many sub-categories on the ScienceQA benchmark.      ",
    "date": "Submitted on 3 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.01139",
    "pdf_link": "https://arxiv.org/pdf/2307.01139",
    "prompted": true
  },
  {
    "title": "Title:TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition",
    "authors": "Authors:Mingxue Xu, Yao Lei Xu, Danilo P. Mandic",
    "abstract": " Abstract:  High-dimensional token embeddings underpin Large Language Models (LLMs), as they can capture subtle semantic information and significantly enhance the modelling of complex language patterns. However, the associated high dimensionality also introduces considerable model parameters, and a prohibitively high model storage. To address this issue, this work proposes an approach based on the Tensor-Train Decomposition (TTD), where each token embedding is treated as a Matrix Product State (MPS) that can be efficiently computed in a distributed manner. The experimental results on GPT-2 demonstrate that, through our approach, the embedding layer can be compressed by a factor of up to 38.40 times, and when the compression factor is 3.31 times, even produced a better performance than the original GPT-2 model.      ",
    "date": "Submitted on 2 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.00526",
    "pdf_link": "https://arxiv.org/pdf/2307.00526",
    "prompted": true
  },
  {
    "title": "Title:Large Language Models Enable Few-Shot Clustering",
    "authors": "Authors:Vijay Viswanathan, Kiril Gashteovski, Carolin Lawrence, Tongshuang Wu, Graham Neubig",
    "abstract": " Abstract:  Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.      ",
    "date": "Submitted on 2 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.00524",
    "pdf_link": "https://arxiv.org/pdf/2307.00524",
    "prompted": true
  },
  {
    "title": "Title:PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation",
    "authors": "Authors:Le Xiao, Xin Shan",
    "abstract": " Abstract:  Large language models(LLMs) have shown excellent text generation capabilities, but there is still much space for improvement in accuracy, sometimes with grammatical errors, semantic inaccuracies, and contextual incoherence, which seriously affect the reliability of the models. These problems may originate from the difficulties and limitations encountered in the pattern extraction stage of large language models. How to utilize the generative power of large language models to generate as many possible patterns that help solve problems and find the optimal patterns from them, so as to use patterns to guide large language models to generate good content, has become a current research hotspot. In this paper, we propose a pattern extraction and selection framework, PatternGPT, which generates rich patterns through the extraction ability of large language models and draws on the idea of federation learning, where multiple agents collaborate with each other to generate diverse patterns. High-quality patterns are selected by defining criteria and optimization algorithms to personalize the guidance of the model generation process. PatternGPT has the advantages of generating diverse and useful patterns, extending relevant knowledge, facilitating efficient pattern use and transfer, and optimizing the quality of generated results and user experience, which provides an effective method for optimizing the text generation capability of large language models and is expected to drive further development in the field of intelligent dialogue and content generation. It is expected to promote further development in the field of intelligent dialogue and content generation.      ",
    "date": "Submitted on 2 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.00470",
    "pdf_link": "https://arxiv.org/pdf/2307.00470",
    "prompted": true
  },
  {
    "title": "Title:Conformer LLMs -- Convolution Augmented Large Language Models",
    "authors": "Authors:Prateek Verma",
    "abstract": " Abstract:  This work builds together two popular blocks of neural architecture, namely convolutional layers and Transformers, for large language models (LLMs). Non-causal conformers are used ubiquitously in automatic speech recognition. This work aims to adapt these architectures in a causal setup for training LLMs. Transformers decoders effectively capture long-range dependencies over several modalities and form a core backbone of modern advancements in machine learning. Convolutional architectures have been popular in extracting features in domains such as raw 1-D signals, speech, and images, to name a few. In this paper, by combining local and global dependencies over latent representations using causal convolutional filters and Transformer, we achieve significant gains in performance. This work showcases a robust speech architecture that can be integrated and adapted in a causal setup beyond speech applications for large-scale language modeling.      ",
    "date": "Submitted on 2 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.00461",
    "pdf_link": "https://arxiv.org/pdf/2307.00461",
    "prompted": true
  },
  {
    "title": "Title:Personality Traits in Large Language Models",
    "authors": "Authors:Mustafa Safdari, Greg Serapio-Garc\u00eda, Cl\u00e9ment Crepy, Stephen Fitz, Peter Romero, Luning Sun, Marwa Abdulhai, Aleksandra Faust, Maja Matari\u0107",
    "abstract": " Abstract:  The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As LLMs increasingly power conversational agents, the synthesized personality embedded in these models by virtue of their training on large amounts of human-generated data draws attention. Since personality is an important factor determining the effectiveness of communication, we present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used LLMs. We find that: 1) personality simulated in the outputs of some LLMs (under specific prompting configurations) is reliable and valid; 2) evidence of reliability and validity of LLM-simulated personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific personality profiles. We also discuss potential applications and ethical implications of our measurement and shaping framework, especially regarding responsible use of LLMs.      ",
    "date": "Submitted on 1 Jul 2023",
    "abstract_link": "https://arxiv.org/abs/2307.00184",
    "pdf_link": "https://arxiv.org/pdf/2307.00184",
    "prompted": true
  },
  {
    "title": "Title:Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models",
    "authors": "Authors:Yiming Wang, Zhuosheng Zhang, Rui Wang",
    "abstract": " Abstract:  Symbolization methods in large language models (LLMs) have been shown effective to improve LLMs' reasoning ability. However, most of these approaches hinge on mapping natural languages to formal languages (e.g., Python, SQL) that are more syntactically complete and free of ambiguity. Although effective, they depart from the natural language itself and deviate from the habits of human thinking, and instead cater more to the execution mindset of computers. In contrast, we hope to simplify natural language by starting from the concept of symbols in linguistics itself, so that LLMs can learn the common formulation and general solution of reasoning problems wrapped in different natural semantics. From this consideration, we propose \\textbf{Meta-Reasoning}, which allows LLMs to automatically accomplish semantic-symbol deconstruction, i.e., semantic resolution, to maximally reduce different questions of certain reasoning tasks to similar natural language representation, thus gaining the ability to learn by analogy and facilitating data-efficient in-context learning. Our experiments show that the Meta-Reasoning paradigm saliently enhances LLMs' reasoning performance with fewer demonstrations. They can learn not only reasoning chains but also general solutions to certain types of tasks. In particular, for symbolic reasoning tasks, such as 7-step Tracking Shuffled Objects, GPT-3 (text-davinci-002) achieves over 99% accuracy with only one Meta-Reasoning demonstration, outperforming all current LLMs with the standard chain-of-thought prompting.      ",
    "date": "Submitted on 30 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17820",
    "pdf_link": "https://arxiv.org/pdf/2306.17820",
    "prompted": true
  },
  {
    "title": "Title:GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models",
    "authors": "Authors:Pawan Kumar Rajpoot, Ankur Parikh",
    "abstract": " Abstract:  Relation extraction (RE) is a crucial task in natural language processing (NLP) that aims to identify and classify relationships between entities mentioned in text. In the financial domain, relation extraction plays a vital role in extracting valuable information from financial documents, such as news articles, earnings reports, and company filings. This paper describes our solution to relation extraction on one such dataset REFinD. The dataset was released along with shared task as a part of the Fourth Workshop on Knowledge Discovery from Unstructured Data in Financial Services, co-located with SIGIR 2023. In this paper, we employed OpenAI models under the framework of in-context learning (ICL). We utilized two retrieval strategies to find top K relevant in-context learning demonstrations / examples from training data for a given test example. The first retrieval mechanism, we employed, is a learning-free dense retriever and the other system is a learning-based retriever. We were able to achieve 4th rank on the leaderboard. Our best F1-score is 0.718.      ",
    "date": "Submitted on 30 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17519",
    "pdf_link": "https://arxiv.org/pdf/2306.17519",
    "prompted": true
  },
  {
    "title": "Title:Preference Ranking Optimization for Human Alignment",
    "authors": "Authors:Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang",
    "abstract": " Abstract:  Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secur AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment by combining a reward model, typically based on Bradley-Terry paired comparison, with an RL algorithm such as Proximal Policy Optimization (PPO) to optimize LLM responses. However, RLHF exhibits complexity, instability, and sensitivity to hyperparameters. In this paper, we propose Preference Ranking Optimization (PRO) as an alternative to PPO for directly aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise Bradley-Terry comparison to accommodate preference rankings of any length. By iteratively contrasting the likelihood of generating responses, PRO instructs the LLM to prioritize the best response while progressively ranking the remaining responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of $n$ responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms existing alignment algorithms, achieving comparable results to ChatGPT and human responses through automatic-based, reward-based, GPT-4, and human evaluations. Furthermore, we demonstrate that longer, more diverse, and higher-quality preference ranking sequences can consistently enhance the performance of human alignment.      ",
    "date": "Submitted on 30 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17492",
    "pdf_link": "https://arxiv.org/pdf/2306.17492",
    "prompted": true
  },
  {
    "title": "Title:Provable Robust Watermarking for AI-Generated Text",
    "authors": "Authors:Xuandong Zhao, Prabhanjan Ananth, Lei Li, Yu-Xiang Wang",
    "abstract": " Abstract:  As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial. To address this challenge, we present GPTWatermark, a robust and high-quality solution designed to ascertain whether a piece of text originates from a specific model. Our approach extends existing watermarking strategies and employs a fixed group design to enhance robustness against editing and paraphrasing attacks. We show that our watermarked language model enjoys strong provable guarantees on generation quality, correctness in detection, and security against evasion attacks. Experimental results on various large language models (LLMs) and diverse datasets demonstrate that our method achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.      ",
    "date": "Submitted on 30 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17439",
    "pdf_link": "https://arxiv.org/pdf/2306.17439",
    "prompted": true
  },
  {
    "title": "Title:SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs",
    "authors": "Authors:Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang",
    "abstract": " Abstract:  In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.      ",
    "date": "Submitted on 30 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17842",
    "pdf_link": "https://arxiv.org/pdf/2306.17842",
    "prompted": true
  },
  {
    "title": "Title:Statler: State-Maintaining Language Models for Embodied Reasoning",
    "authors": "Authors:Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, Matthew R. Walter",
    "abstract": " Abstract:  Large language models (LLMs) provide a promising tool that enable robots to perform complex robot reasoning tasks. However, the limited context window of contemporary LLMs makes reasoning over long time horizons difficult. Embodied tasks such as those that one might expect a household robot to perform typically require that the planner consider information acquired a long time ago (e.g., properties of the many objects that the robot previously encountered in the environment). Attempts to capture the world state using an LLM's implicit internal representation is complicated by the paucity of task- and environment-relevant information available in a robot's action history, while methods that rely on the ability to convey information via the prompt to the LLM are subject to its limited context window. In this paper, we propose Statler, a framework that endows LLMs with an explicit representation of the world state as a form of ``memory'' that is maintained over time. Integral to Statler is its use of two instances of general LLMs -- a world-model reader and a world-model writer -- that interface with and maintain the world state. By providing access to this world state ``memory'', Statler improves the ability of existing LLMs to reason over longer time horizons without the constraint of context length. We evaluate the effectiveness of our approach on three simulated table-top manipulation domains and a real robot domain, and show that it improves the state-of-the-art in LLM-based robot reasoning. Project website: this https URL ",
    "date": "Submitted on 30 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17840",
    "pdf_link": "https://arxiv.org/pdf/2306.17840",
    "prompted": true
  },
  {
    "title": "Title:Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting",
    "authors": "Authors:Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, Michael Bendersky",
    "abstract": " Abstract:  Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, there has been limited success so far, as researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these ranking formulations, possibly due to the nature of how LLMs are trained. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on the Flan-UL2 model with 20B parameters outperforms the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, by over 5% at NDCG@1. On TREC-DL2019, PRP is only inferior to the GPT-4 solution on the NDCG@5 and NDCG@10 metrics, while outperforming other existing solutions, such as InstructGPT which has 175B parameters, by over 10% for nearly all ranking metrics. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity. We also discuss other benefits of PRP, such as supporting both generation and scoring LLM APIs, as well as being insensitive to input ordering.      ",
    "date": "Submitted on 30 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17563",
    "pdf_link": "https://arxiv.org/pdf/2306.17563",
    "prompted": true
  },
  {
    "title": "Title:Harnessing LLMs in Curricular Design: Using GPT-4 to Support Authoring of Learning Objectives",
    "authors": "Authors:Pragnya Sridhar, Aidan Doyle, Arav Agarwal, Christopher Bogart, Jaromir Savelka, Majd Sakr",
    "abstract": " Abstract:  We evaluated the capability of a generative pre-trained transformer (GPT-4) to automatically generate high-quality learning objectives (LOs) in the context of a practically oriented university course on Artificial Intelligence. Discussions of opportunities (e.g., content generation, explanation) and risks (e.g., cheating) of this emerging technology in education have intensified, but to date there has not been a study of the models' capabilities in supporting the course design and authoring of LOs. LOs articulate the knowledge and skills learners are intended to acquire by engaging with a course. To be effective, LOs must focus on what students are intended to achieve, focus on specific cognitive processes, and be measurable. Thus, authoring high-quality LOs is a challenging and time consuming (i.e., expensive) effort. We evaluated 127 LOs that were automatically generated based on a carefully crafted prompt (detailed guidelines on high-quality LOs authoring) submitted to GPT-4 for conceptual modules and projects of an AI Practitioner course. We analyzed the generated LOs if they follow certain best practices such as beginning with action verbs from Bloom's taxonomy in regards to the level of sophistication intended. Our analysis showed that the generated LOs are sensible, properly expressed (e.g., starting with an action verb), and that they largely operate at the appropriate level of Bloom's taxonomy, respecting the different nature of the conceptual modules (lower levels) and projects (higher levels). Our results can be leveraged by instructors and curricular designers wishing to take advantage of the state-of-the-art generative models to support their curricular and course design efforts.      ",
    "date": "Submitted on 30 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17459",
    "pdf_link": "https://arxiv.org/pdf/2306.17459",
    "prompted": true
  },
  {
    "title": "Title:Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models",
    "authors": "Authors:Harnoor Dhingra, Preetiha Jayashanker, Sayali Moghe, Emma Strubell",
    "abstract": " Abstract:  Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.      ",
    "date": "Submitted on 30 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2307.00101",
    "pdf_link": "https://arxiv.org/pdf/2307.00101",
    "prompted": true
  },
  {
    "title": "Title:LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT",
    "authors": "Authors:Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenhu Chen, Wei Xue, Yike Guo",
    "abstract": " Abstract:  We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the \"ear\" by transcribing the audio, while GPT-4 serves as the \"brain,\" acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a human-annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task.      ",
    "date": "Submitted on 29 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17103",
    "pdf_link": "https://arxiv.org/pdf/2306.17103",
    "prompted": true
  },
  {
    "title": "Title:UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?",
    "authors": "Authors:Junda Wang, Zonghai Yao, Avijit Mitra, Samuel Osebe, Zhichao Yang, Hong Yu",
    "abstract": " Abstract:  This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023 shared task for Task-A and Task-C. We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets. The experiment results demonstrate that our approaches yield reasonable performance as evaluated by automatic metrics such as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we conducted a comparative analysis between our proposed method and ChatGPT and GPT-4. This analysis also investigates the potential of utilizing cooperation LLMs to generate high-quality datasets.      ",
    "date": "Submitted on 29 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16931",
    "pdf_link": "https://arxiv.org/pdf/2306.16931",
    "prompted": true
  },
  {
    "title": "Title:Benchmarking Large Language Model Capabilities for Conditional Generation",
    "authors": "Authors:Joshua Maynez, Priyanka Agrawal, Sebastian Gehrmann",
    "abstract": " Abstract:  Pre-trained large language models (PLMs) underlie most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside techniques like few-shot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks--while they can be used to compare systems at a high level--relate to the real world use cases for which people have been adopting them. In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input and output language. Our results show that PLMs differ in their applicability to different data regimes and their generalization to multiple languages and inform which PLMs to use for a given generation task setup. We share best practices to be taken into consideration when benchmarking generation capabilities during the development of upcoming PLMs.      ",
    "date": "Submitted on 29 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16793",
    "pdf_link": "https://arxiv.org/pdf/2306.16793",
    "prompted": true
  },
  {
    "title": "Title:CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?",
    "authors": "Authors:Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, Bin Wang",
    "abstract": " Abstract:  We present the Chinese Elementary School Math Word Problems (CMATH) dataset, comprising 1.7k elementary school-level math word problems with detailed annotations, source from actual Chinese workbooks and exams. This dataset aims to provide a benchmark tool for assessing the following question: to what grade level of elementary school math do the abilities of popular large language models (LLMs) correspond? We evaluate a variety of popular LLMs, including both commercial and open-source options, and discover that only GPT-4 achieves success (accuracy $\\geq$ 60\\%) across all six elementary school grades, while other models falter at different grade levels. Furthermore, we assess the robustness of several top-performing LLMs by augmenting the original problems in the CMATH dataset with distracting information. Our findings reveal that GPT-4 is able to maintains robustness, while other model fail. We anticipate that our study will expose limitations in LLMs' arithmetic and reasoning capabilities, and promote their ongoing development and advancement.      ",
    "date": "Submitted on 29 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16636",
    "pdf_link": "https://arxiv.org/pdf/2306.16636",
    "prompted": true
  },
  {
    "title": "Title:Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors",
    "authors": "Authors:Tung Phung, Victor-Alexandru P\u0103durean, Jos\u00e9 Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, Gustavo Soares",
    "abstract": " Abstract:  Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors' performance for several scenarios. These results also highlight settings where GPT-4 still struggles, providing exciting future directions on developing techniques to improve the performance of these models.      ",
    "date": "Submitted on 29 Jun 2023 (v1), last revised 30 Jun 2023 (this version, v2)",
    "abstract_link": "https://arxiv.org/abs/2306.17156",
    "pdf_link": "https://arxiv.org/pdf/2306.17156"
  },
  {
    "title": "Title:LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding",
    "authors": "Authors:Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, Tong Sun",
    "abstract": " Abstract:  Instruction tuning unlocks the superior capability of Large Language Models (LLM) to interact with humans. Furthermore, recent instruction-following datasets include images as visual inputs, collecting responses for image-based instructions. However, visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first use publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Moreover, we prompt text-only GPT-4 with recognized texts and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multi-modal instruction-following data, our model, LLaVAR, substantially improves the LLaVA model's capability on text-based VQA datasets (up to 20% accuracy improvement) while achieving an accuracy of 91.42% on ScienceQA. The GPT-4-based instruction-following evaluation also demonstrates the improvement of our model on both natural images and text-rich images. Through qualitative analysis, LLaVAR shows promising interaction (e.g., reasoning, writing, and elaboration) skills with humans based on the latest real-world online content that combines text and images. We make our code/data/models publicly available at this https URL.      ",
    "date": "Submitted on 29 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17107",
    "pdf_link": "https://arxiv.org/pdf/2306.17107"
  },
  {
    "title": "Title:Concept-Oriented Deep Learning with Large Language Models",
    "authors": "Authors:Daniel T. Chang",
    "abstract": " Abstract:  Large Language Models (LLMs) have been successfully used in many natural-language tasks and applications including text generation and AI chatbots. They also are a promising new technology for concept-oriented deep learning (CODL). However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency. We discuss these in this paper, as well as major uses of LLMs for CODL including concept extraction from text, concept graph extraction from text, and concept learning. Human knowledge consists of both symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal LLMs, on the other hand, are capable of representing the full range (conceptual and sensory) of human knowledge. We discuss conceptual understanding in visual-language LLMs, the most important multimodal LLMs, and major uses of them for CODL including concept extraction from image, concept graph extraction from image, and concept learning. While uses of LLMs for CODL are valuable standalone, they are particularly valuable as part of LLM applications such as AI chatbots.      ",
    "date": "Submitted on 29 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17089",
    "pdf_link": "https://arxiv.org/pdf/2306.17089"
  },
  {
    "title": "Title:The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps",
    "authors": "Authors:Jina Kim, Zekun Li, Yijun Lin, Min Namgung, Leeje Jang, Yao-Yi Chiang",
    "abstract": " Abstract:  Documents hold spatial focus and valuable locality characteristics. For example, descriptions of listings in real estate or travel blogs contain information about specific local neighborhoods. This information is valuable to characterize how humans perceive their environment. However, the first step to making use of this information is to identify the spatial focus (e.g., a city) of a document. Traditional approaches for identifying the spatial focus of a document rely on detecting and disambiguating toponyms from the document. This approach requires a vocabulary set of location phrases and ad-hoc rules, which ignore important words related to location. Recent topic modeling approaches using large language models often consider a few topics, each with broad coverage. In contrast, the spatial focus of a document can be a country, a city, or even a neighborhood, which together, is much larger than the number of topics considered in these approaches. Additionally, topic modeling methods are often applied to broad topics of news articles where context is easily distinguishable. To identify the geographic focus of a document effectively, we present a simple but effective Joint Embedding of multi-LocaLitY (JELLY), which jointly learns representations with separate encoders of document and location. JELLY significantly outperforms state-of-the-art methods for identifying spatial focus from documents from a number of sources. We also demonstrate case studies on the arithmetic of the learned representations, including identifying cities with similar locality characteristics and zero-shot learning to identify document spatial focus.      ",
    "date": "Submitted on 29 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17059",
    "pdf_link": "https://arxiv.org/pdf/2306.17059"
  },
  {
    "title": "Title:Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language",
    "authors": "Authors:William Berrios, Gautam Mittal, Tristan Thrush, Douwe Kiela, Amanpreet Singh",
    "abstract": "Abstract:  We propose LENS, a modular approach for tackling computer vision problems byleveraging the power of large language models (LLMs). Our system uses alanguage model to reason over outputs from a set of independent and highlydescriptive vision modules that provide exhaustive information about an image.We evaluate the approach on pure computer vision settings such as zero- andfew-shot object recognition, as well as on vision and language problems. LENScan be applied to any off-the-shelf LLM and we find that the LLMs with LENSperform highly competitively with much bigger and much more sophisticatedsystems, without any multimodal training whatsoever. We open-source our code atthis https URL and provide an interactive demo.    ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16410",
    "pdf_link": "https://arxiv.org/pdf/2306.16410"
  },
  {
    "title": "Title:Towards Measuring the Representation of Subjective Global Opinions in Language Models",
    "authors": "Authors:Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, Deep Ganguli",
    "abstract": "Abstract:  Large language models (LLMs) may not equitably represent diverse globalperspectives on societal issues. In this paper, we develop a quantitativeframework to evaluate whose opinions model-generated responses are more similarto. We first build a dataset, GlobalOpinionQA, comprised of questions andanswers from cross-national surveys designed to capture diverse opinions onglobal issues across different countries. Next, we define a metric thatquantifies the similarity between LLM-generated survey responses and humanresponses, conditioned on country. With our framework, we run three experimentson an LLM trained to be helpful, honest, and harmless with Constitutional AI.By default, LLM responses tend to be more similar to the opinions of certainpopulations, such as those from the USA, and some European and South Americancountries, highlighting the potential for biases. When we prompt the model toconsider a particular country's perspective, responses shift to be more similarto the opinions of the prompted populations, but can reflect harmful culturalstereotypes. When we translate GlobalOpinionQA questions to a target language,the model's responses do not necessarily become the most similar to theopinions of speakers of those languages. We release our dataset for others touse and build on. Our data is atthis https URL. We also providean interactive visualization at this https URL.    ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16388",
    "pdf_link": "https://arxiv.org/pdf/2306.16388"
  },
  {
    "title": "Title:Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models",
    "authors": "Authors:Zaid Alyafeai, Maged S. Alshaibani, Badr AlKhamissi, Hamzah Luqman, Ebrahim Alareqi, Ali Fadel",
    "abstract": "Abstract:  Large language models (LLMs) have demonstrated impressive performance onvarious downstream tasks without requiring fine-tuning, including ChatGPT, achat-based model built on top of LLMs such as GPT-3.5 and GPT-4. Despite havinga lower training proportion compared to English, these models also exhibitremarkable capabilities in other languages. In this study, we assess theperformance of GPT-3.5 and GPT-4 models on seven distinct Arabic NLP tasks:sentiment analysis, translation, transliteration, paraphrasing, part of speechtagging, summarization, and diacritization. Our findings reveal that GPT-4outperforms GPT-3.5 on five out of the seven tasks. Furthermore, we conduct anextensive analysis of the sentiment analysis task, providing insights into howLLMs achieve exceptional results on a challenging dialectal dataset.Additionally, we introduce a new Python interfacethis https URL that facilitates the evaluation of these taskseffortlessly.    ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16322",
    "pdf_link": "https://arxiv.org/pdf/2306.16322"
  },
  {
    "title": "Title:Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
    "authors": "Authors:Yiwen Shi, Ping Ren, Jing Wang, Biao Han, Taha ValizadehAslani, Felix Agbavor, Yi Zhang, Meng Hu, Liang Zhao, Hualou Liang",
    "abstract": "Abstract:  Food effect summarization from New Drug Application (NDA) is an essentialcomponent of product-specific guidance (PSG) development and assessment.However, manual summarization of food effect from extensive drug applicationreview documents is time-consuming, which arouses a need to develop automatedmethods. Recent advances in large language models (LLMs) such as ChatGPT andGPT-4, have demonstrated great potential in improving the effectiveness ofautomated text summarization, but its ability regarding the accuracy insummarizing food effect for PSG assessment remains unclear. In this study, weintroduce a simple yet effective approach, iterative prompting, which allowsone to interact with ChatGPT or GPT-4 more effectively and efficiently throughmulti-turn interaction. Specifically, we propose a three-turn iterativeprompting approach to food effect summarization in which the keyword-focusedand length-controlled prompts are respectively provided in consecutive turns torefine the quality of the generated summary. We conduct a series of extensiveevaluations, ranging from automated metrics to FDA professionals and evenevaluation by GPT-4, on 100 NDA review documents selected over the past fiveyears. We observe that the summary quality is progressively improved throughoutthe process. Moreover, we find that GPT-4 performs better than ChatGPT, asevaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%).Importantly, all the FDA professionals unanimously rated that 85% of thesummaries generated by GPT-4 are factually consistent with the golden referencesummary, a finding further supported by GPT-4 rating of 72% consistency. Theseresults strongly suggest a great potential for GPT-4 to draft food effectsummaries that could be reviewed by FDA professionals, thereby improving theefficiency of PSG assessment cycle and promoting the generic drug productdevelopment.    ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16275",
    "pdf_link": "https://arxiv.org/pdf/2306.16275"
  },
  {
    "title": "Title:CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models",
    "authors": "Authors:Yufei Huang, Deyi Xiong",
    "abstract": "Abstract:  Holistically measuring societal biases of large language models is crucialfor detecting and reducing ethical risks in highly capable AI models. In thiswork, we present a Chinese Bias Benchmark dataset that consists of over 100Kquestions jointly constructed by human experts and generative language models,covering stereotypes and societal biases in 14 social dimensions related toChinese culture and values. The curation process contains 4 essential steps:bias identification via extensive literature review, ambiguous contextgeneration, AI-assisted disambiguous context generation, snd manual review \\&recomposition. The testing instances in the dataset are automatically derivedfrom 3K+ high-quality templates manually authored with stringent qualitycontrol. The dataset exhibits wide coverage and high diversity. Extensiveexperiments demonstrate the effectiveness of the dataset in detecting modelbias, with all 10 publicly available Chinese large language models exhibitingstrong bias in certain categories. Additionally, we observe from ourexperiments that fine-tuned models could, to a certain extent, heedinstructions and avoid generating outputs that are morally harmful in sometypes, in the way of \"moral self-correction\". Our dataset and results arepublicly available at\\href{this https URL}{this https URL},offering debiasing research opportunities to a widened community.    ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16244",
    "pdf_link": "https://arxiv.org/pdf/2306.16244"
  },
  {
    "title": "Title:Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks",
    "authors": "Authors:Samy Ateia, Udo Kruschwitz",
    "abstract": "Abstract:  We assessed the performance of commercial Large Language Models (LLMs)GPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11bPhase B, which is focused on answer generation, both models demonstratedcompetitive abilities with leading systems. Remarkably, they achieved this withsimple zero-shot learning, grounded with relevant snippets. Even withoutrelevant snippets, their performance was decent, though not on par with thebest systems. Interestingly, the older and cheaper GPT-3.5-Turbo system wasable to compete with GPT-4 in the grounded Q&A setting on factoid and listanswers. In Task 11b Phase A, focusing on retrieval, query expansion throughzero-shot learning improved performance, but the models fell short compared toother systems. The code needed to rerun these experiments is available throughGitHub.    ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16108",
    "pdf_link": "https://arxiv.org/pdf/2306.16108"
  },
  {
    "title": "Title:ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases",
    "authors": "Authors:Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, Li Yuan",
    "abstract": "Abstract:  Large Language Models (LLMs) have shown the potential to revolutionizenatural language processing tasks in various domains, sparking great interestin vertical-specific large models. However, unlike proprietary models such asBloombergGPT and FinGPT, which have leveraged their unique data accumulationsto make strides in the finance domain, there hasn't not many similar largelanguage models in the Chinese legal domain to facilitate its digitaltransformation.In this paper, we propose an open-source legal large language model namedChatLaw. Due to the importance of data quality, we carefully designed a legaldomain fine-tuning dataset. Additionally, to overcome the problem of modelhallucinations in legal data screening during reference data retrieval, weintroduce a method that combines vector database retrieval with keywordretrieval to effectively reduce the inaccuracy of relying solely on vectordatabase retrieval. Furthermore, we propose a self-attention method to enhancethe ability of large models to overcome errors present in reference data,further optimizing the issue of model hallucinations at the model level andimproving the problem-solving capabilities of large models. We alsoopen-sourced our model and part of the data atthis https URL.    ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16092",
    "pdf_link": "https://arxiv.org/pdf/2306.16092"
  },
  {
    "title": "Title:Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition",
    "authors": "Authors:Yuang Li, Yu Wu, Jinyu Li, Shujie Liu",
    "abstract": "Abstract:  The integration of Language Models (LMs) has proven to be an effective way toaddress domain shifts in speech recognition. However, these approaches usuallyrequire a significant amount of target domain text data for the training ofLMs. Different from these methods, in this work, with only a domain-specifictext prompt, we propose two zero-shot ASR domain adaptation methods usingLLaMA, a 7-billion-parameter large language model (LLM). LLM is used in twoways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASRsystem with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of anencoder-decoder based ASR system. Experiments show that, with only one domainprompt, both methods can effectively reduce word error rates (WER) onout-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deepLLM-fusion has the advantage of better recall of entity and out-of-vocabularywords.    ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16007",
    "pdf_link": "https://arxiv.org/pdf/2306.16007"
  },
  {
    "title": "Title:Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias",
    "authors": "Authors:Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, Chao Zhang",
    "abstract": "Abstract:  Large language models (LLMs) have been recently leveraged as training datagenerators for various natural language processing (NLP) tasks. While previousresearch has explored different approaches to training models using generateddata, they generally rely on simple class-conditional prompts, which may limitthe diversity of the generated data and inherit systematic biases of LLM. Thus,we investigate training data generation with diversely attributed prompts(e.g., specifying attributes like length and style), which have the potentialto yield diverse and attributed generated data. Our investigation focuses ondatasets with high cardinality and diverse domains, wherein we demonstrate thatattributed prompts outperform simple class-conditional prompts in terms of theresulting model's performance. Additionally, we present a comprehensiveempirical study on data generation encompassing vital aspects like bias,diversity, and efficiency, and highlight three key observations: firstly,synthetic datasets generated by simple prompts exhibit significant biases, suchas regional bias; secondly, attribute diversity plays a pivotal role inenhancing model performance; lastly, attributed prompts achieve the performanceof simple class-conditional prompts while utilizing only 5\\% of the queryingcost of ChatGPT associated with the latter. We release the generated datasetand used prompts to facilitate future research. The data and code will beavailable on \\url{this https URL}.    ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.15895",
    "pdf_link": "https://arxiv.org/pdf/2306.15895"
  },
  {
    "title": "Title:On the Exploitability of Instruction Tuning",
    "authors": "Authors:Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, Tom Goldstein",
    "abstract": " Abstract:  Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \\textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs. Code is available at \\url{this https URL}.      ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17194",
    "pdf_link": "https://arxiv.org/pdf/2306.17194"
  },
  {
    "title": "Title:Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision",
    "authors": "Authors:Theodore Zhao, Mu Wei, J. Samuel Preston, Hoifung Poon",
    "abstract": " Abstract:  Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in significant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.      ",
    "date": "Submitted on 28 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.16564",
    "pdf_link": "https://arxiv.org/pdf/2306.16564"
  },
  {
    "title": "Title:Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese",
    "authors": "Authors:Maria Carolina Penteado, F\u00e1bio Perez",
    "abstract": "Abstract:  We investigate the effectiveness of GPT-3.5 and GPT-4, two large languagemodels, as Grammatical Error Correction (GEC) tools for Brazilian Portugueseand compare their performance against Microsoft Word and Google Docs. Weintroduce a GEC dataset for Brazilian Portuguese with four categories: Grammar,Spelling, Internet, and Fast typing. Our results show that while GPT-4 hashigher recall than other methods, LLMs tend to have lower precision, leading toovercorrection. This study demonstrates the potential of LLMs as practical GECtools for Brazilian Portuguese and encourages further exploration of LLMs fornon-English languages and other educational settings.    ",
    "date": "Submitted on 27 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.15788",
    "pdf_link": "https://arxiv.org/pdf/2306.15788"
  },
  {
    "title": "Title:Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost",
    "authors": "Authors:Parikshit Bansal, Amit Sharma",
    "abstract": "Abstract:  State-of-the-art supervised NLP models achieve high accuracy but are alsosusceptible to failures on inputs from low-data regimes, such as domains thatare not represented in training data. As an approximation to collectingground-truth labels for the specific domain, we study the use of large languagemodels (LLMs) for annotating inputs and improving the generalization of NLPmodels. Specifically, given a budget for LLM annotations, we present analgorithm for sampling the most informative inputs to annotate and retrain theNLP model. We find that popular active learning strategies such asuncertainty-based sampling do not work well. Instead, we propose a samplingstrategy based on the difference in prediction scores between the base modeland the finetuned NLP model, utilizing the fact that most NLP models arefinetuned from a base model. Experiments with classification (semanticsimilarity) and ranking (semantic search) tasks show that our sampling strategyleads to significant gains in accuracy for both the training and targetdomains.    ",
    "date": "Submitted on 27 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.15766",
    "pdf_link": "https://arxiv.org/pdf/2306.15766"
  },
  {
    "title": "Title:Extending Context Window of Large Language Models via Positional Interpolation",
    "authors": "Authors:Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian",
    "abstract": "Abstract:  We present Position Interpolation (PI) that extends the context window sizesof RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimalfine-tuning (within 1000 steps), while demonstrating strong empirical resultson various tasks that require long context, including passkey retrieval,language modeling, and long document summarization from LLaMA 7B to 65B.Meanwhile, the extended model by Position Interpolation preserve qualityrelatively well on tasks within its original context window. To achieve thisgoal, Position Interpolation linearly down-scales the input position indices tomatch the original context window size, rather than extrapolating beyond thetrained context length which may lead to catastrophically high attention scoresthat completely ruin the self-attention mechanism. Our theoretical study showsthat the upper bound of interpolation is at least $\\sim 600 \\times$ smallerthan that of extrapolation, further demonstrating its stability. Modelsextended via Position Interpolation retain its original architecture and canreuse most pre-existing optimization and infrastructure.    ",
    "date": "Submitted on 27 Jun 2023 (v1), last revised 28 Jun 2023 (this version, v2)",
    "abstract_link": "https://arxiv.org/abs/2306.15595",
    "pdf_link": "https://arxiv.org/pdf/2306.15595"
  },
  {
    "title": "Title:Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool",
    "authors": "Authors:Jingwei Ni, Julia Bingler, Chiara Colesanti-Senni, Mathias Kraus, Glen Gostlow, Tobias Schimanski, Dominik Stammbach, Saeid Ashraf Vaghefi, Qian Wang, Nicolas Webersinke, Tobias Wekhof, Tingyu Yu, Markus Leippold",
    "abstract": "Abstract:  This paper introduces a novel approach to enhance Large Language Models(LLMs) with expert knowledge to automate the analysis of corporatesustainability reports by benchmarking them against the Task Force forClimate-Related Financial Disclosures (TCFD) recommendations. Corporatesustainability reports are crucial in assessing organizations' environmentaland social risks and impacts. However, analyzing these reports' vast amounts ofinformation makes human analysis often too costly. As a result, only a fewentities worldwide have the resources to analyze these reports, which couldlead to a lack of transparency. While AI-powered tools can automaticallyanalyze the data, they are prone to inaccuracies as they lack domain-specificexpertise. This paper introduces a novel approach to enhance LLMs with expertknowledge to automate the analysis of corporate sustainability reports. Wechristen our tool CHATREPORT, and apply it in a first use case to assesscorporate climate risk disclosures following the TCFD recommendations.CHATREPORT results from collaborating with experts in climate science, finance,economic policy, and computer science, demonstrating how domain experts can beinvolved in developing AI tools. We make our prompt templates, generated data,and scores available to the public to encourage transparency.    ",
    "date": "Submitted on 27 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.15518",
    "pdf_link": "https://arxiv.org/pdf/2306.15518"
  },
  {
    "title": "Title:Using Large Language Models to Provide Explanatory Feedback to Human Tutors",
    "authors": "Authors:Jionghao Lin, Danielle R. Thomas, Feifei Han, Shivang Gupta, Wei Tan, Ngoc Dang Nguyen, Kenneth R. Koedinger",
    "abstract": "Abstract:  Research demonstrates learners engaging in the process of producingexplanations to support their reasoning, can have a positive impact onlearning. However, providing learners real-time explanatory feedback oftenpresents challenges related to classification accuracy, particularly indomain-specific environments, containing situationally complex and nuancedresponses. We present two approaches for supplying tutors real-time feedbackwithin an online lesson on how to give students effective praise. Thiswork-in-progress demonstrates considerable accuracy in binary classificationfor corrective feedback of effective, or effort-based (F1 score = 0.811), andineffective, or outcome-based (F1 score = 0.350), praise responses. Morenotably, we introduce progress towards an enhanced approach of providingexplanatory feedback using large language model-facilitated named entityrecognition, which can provide tutors feedback, not only while engaging inlessons, but can potentially suggest real-time tutor moves. Future workinvolves leveraging large language models for data augmentation to improveaccuracy, while also developing an explanatory feedback interface.    ",
    "date": "Submitted on 27 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.15498",
    "pdf_link": "https://arxiv.org/pdf/2306.15498"
  },
  {
    "title": "Title:SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design",
    "authors": "Authors:Fu-Ming Guo",
    "abstract": "Abstract:  This paper introduces SparseOptimizer, a novel deep learning optimizer thatexploits Moreau-Yosida regularization to naturally induce sparsity in largelanguage models such as BERT, ALBERT and GPT. Key to the design ofSparseOptimizer is an embedded shrinkage operator, which imparts sparsitydirectly within the optimization process. This operator, backed by a soundtheoretical framework, includes an analytical solution, thereby reinforcing theoptimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-playfunctionality eradicates the need for code modifications, making it auniversally adaptable tool for a wide array of large language models. Empiricalevaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2confirm that SparseBERT and SparseALBERT, when sparsified usingSparseOptimizer, achieve performance comparable to their dense counterparts,BERT and ALBERT, while significantly reducing their parameter count. Further,this work proposes an innovative optimizer-compiler co-design strategy,demonstrating the potential of inference acceleration (\\textbf{3.37x},\\textbf{6.30x}, and \\textbf{7.15x} in comparison with Pytorch, TensorFlow, andLLVM generic compile, respectively) in SparseBERT when paired with anappropriately designed compiler. This study represents a significant stepforward in the evolution of efficient, scalable, and high-performing largelanguage models, setting a precedent for future exploration and optimization inthis domain. The SparseOptimizer code and SparseALBERT model will be madeavailable upon paper acceptance.    ",
    "date": "Submitted on 27 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.15656",
    "pdf_link": "https://arxiv.org/pdf/2306.15656"
  },
  {
    "title": "Title:Are aligned neural networks adversarially aligned?",
    "authors": "Authors:Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, Ludwig Schmidt",
    "abstract": "Abstract:  Large language models are now tuned to align with the goals of theircreators, namely to be \"helpful and harmless.\" These models should respondhelpfully to user questions, but refuse to answer requests that could causeharm. However, adversarial users can construct inputs which circumvent attemptsat alignment. In this work, we study to what extent these models remainaligned, even when interacting with an adversarial user who constructsworst-case inputs (adversarial examples). These inputs are designed to causethe model to emit harmful content that would otherwise be prohibited. We showthat existing NLP-based optimization attacks are insufficiently powerful toreliably attack aligned text models: even when current NLP-based attacks fail,we can find adversarial inputs with brute force. As a result, the failure ofcurrent attacks should not be seen as proof that aligned text models remainaligned under adversarial inputs.However the recent trend in large-scale ML models is multimodal models thatallow users to provide images that influence the text that is generated. Weshow these models can be easily attacked, i.e., induced to perform arbitraryun-aligned behavior through adversarial perturbation of the input image. Weconjecture that improved NLP attacks may demonstrate this same level ofadversarial control over text-only models.    ",
    "date": "Submitted on 26 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.15447",
    "pdf_link": "https://arxiv.org/pdf/2306.15447"
  },
  {
    "title": "Title:WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
    "authors": "Authors:Virginia K. Felkner, Ho-Chun Herbert Chang, Eugene Jang, Jonathan May",
    "abstract": "Abstract:  We present WinoQueer: a benchmark specifically designed to measure whetherlarge language models (LLMs) encode biases that are harmful to the LGBTQ+community. The benchmark is community-sourced, via application of a novelmethod that generates a bias benchmark from a community survey. We apply ourbenchmark to several popular LLMs and find that off-the-shelf models generallydo exhibit considerable anti-queer bias. Finally, we show that LLM bias againsta marginalized community can be somewhat mitigated by finetuning on datawritten about or by members of that community, and that social media textwritten by community members is more effective than news text written about thecommunity by non-members. Our method for community-in-the-loop benchmarkdevelopment provides a blueprint for future researchers to developcommunity-driven, harms-grounded LLM benchmarks for other marginalizedcommunities.    ",
    "date": "Submitted on 26 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.15087",
    "pdf_link": "https://arxiv.org/pdf/2306.15087"
  },
  {
    "title": "Title:InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
    "authors": "Authors:John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao",
    "abstract": "Abstract:  Humans write code in a fundamentally interactive manner and rely on constantexecution feedback to correct errors, resolve ambiguities, and decompose tasks.While LLMs have recently exhibited promising coding capabilities, currentcoding benchmarks mostly consider a static instruction-to-code sequencetransduction process, which has the potential for error propagation and adisconnect between the generated code and its final execution environment. Toaddress this gap, we introduce InterCode, a lightweight, flexible, andeasy-to-use framework of interactive coding as a standard reinforcementlearning (RL) environment, with code as actions and execution feedback asobservations. Our framework is language and platform agnostic, usesself-contained Docker environments to provide safe and reproducible execution,and is compatible out-of-the-box with traditional seq2seq coding methods, whileenabling the development of new methods for interactive code generation. We useInterCode to create two interactive code environments with Bash and SQL asaction spaces, leveraging data from the static Spider and NL2Bash datasets. Wedemonstrate InterCode's viability as a testbed by evaluating multiplestate-of-the-art LLMs configured with different prompting strategies such asReAct and Plan & Solve. Our results showcase the benefits of interactive codegeneration and demonstrate that InterCode can serve as a challenging benchmarkfor advancing code understanding and generation capabilities. InterCode isdesigned to be easily extensible and can even be used to incorporate new taskssuch as Capture the Flag, a popular coding puzzle that is inherently multi-stepand involves multiple programming languages. Project site with code and data:this https URL",
    "date": "Submitted on 26 Jun 2023 (v1), last revised 27 Jun 2023 (this version, v2)",
    "abstract_link": "https://arxiv.org/abs/2306.14898",
    "pdf_link": "https://arxiv.org/pdf/2306.14898"
  },
  {
    "title": "Title:Kosmos-2: Grounding Multimodal Large Language Models to the World",
    "authors": "Authors:Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei",
    "abstract": "Abstract:  We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling newcapabilities of perceiving object descriptions (e.g., bounding boxes) andgrounding text to the visual world. Specifically, we represent referexpressions as links in Markdown, i.e., ``[text span](bounding boxes)'', whereobject descriptions are sequences of location tokens. Together with multimodalcorpora, we construct large-scale data of grounded image-text pairs (calledGrIT) to train the model. In addition to the existing capabilities of MLLMs(e.g., perceiving general modalities, following instructions, and performingin-context learning), Kosmos-2 integrates the grounding capability intodownstream applications. We evaluate Kosmos-2 on a wide range of tasks,including (i) multimodal grounding, such as referring expression comprehension,and phrase grounding, (ii) multimodal referring, such as referring expressiongeneration, (iii) perception-language tasks, and (iv) language understandingand generation. This work lays out the foundation for the development ofEmbodiment AI and sheds light on the big convergence of language, multimodalperception, action, and world modeling, which is a key step toward artificialgeneral intelligence. Data, demo, and pretrained models are available atthis https URL.    ",
    "date": "Submitted on 26 Jun 2023 (v1), last revised 27 Jun 2023 (this version, v2)",
    "abstract_link": "https://arxiv.org/abs/2306.14824",
    "pdf_link": "https://arxiv.org/pdf/2306.14824"
  },
  {
    "title": "Title:Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement",
    "authors": "Authors:Hang Dong, Jiaoyan Chen, Yuan He, Ian Horrocks",
    "abstract": "Abstract:  Mentions of new concepts appear regularly in texts and require automatedapproaches to harvest and place them into Knowledge Bases (KB), e.g.,ontologies and taxonomies. Existing datasets suffer from three issues, (i)mostly assuming that a new concept is pre-discovered and cannot supportout-of-KB mention discovery; (ii) only using the concept label as the inputalong with the KB and thus lacking the contexts of a concept label; and (iii)mostly focusing on concept placement w.r.t a taxonomy of atomic concepts,instead of complex concepts, i.e., with logical operators. To address theseissues, we propose a new benchmark, adapting MedMentions dataset (PubMedabstracts) with SNOMED CT versions in 2014 and 2017 under the Diseasessub-category and the broader categories of Clinical finding, Procedure, andPharmaceutical / biologic product. We provide usage on the evaluation with thedataset for out-of-KB mention discovery and concept placement, adapting recentLarge Language Model based methods.    ",
    "date": "Submitted on 26 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14704",
    "pdf_link": "https://arxiv.org/pdf/2306.14704"
  },
  {
    "title": "Title:Exploring the Robustness of Large Language Models for Solving Programming Problems",
    "authors": "Authors:Atsushi Shirafuji, Yutaka Watanobe, Takumi Ito, Makoto Morishita, Yuki Nakamura, Yusuke Oda, Jun Suzuki",
    "abstract": "Abstract:  Using large language models (LLMs) for source code has recently gainedattention. LLMs, such as Transformer-based models like Codex and ChatGPT, havebeen shown to be highly capable of solving a wide range of programmingproblems. However, the extent to which LLMs understand problem descriptions andgenerate programs accordingly or just retrieve source code from the mostrelevant problem in training data based on superficial cues has not beendiscovered yet. To explore this research question, we conduct experiments tounderstand the robustness of several popular LLMs, CodeGen and GPT-3.5 seriesmodels, capable of tackling code generation tasks in introductory programmingproblems. Our experimental results show that CodeGen and Codex are sensitive tothe superficial modifications of problem descriptions and significantly impactcode generation performance. Furthermore, we observe that Codex relies onvariable names, as randomized variables decrease the solved rate significantly.However, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT,show higher robustness to superficial modifications and have an outstandingcapability for solving programming problems. This highlights the fact thatslight modifications to the prompts given to the LLMs can greatly affect codegeneration performance, and careful formatting of prompts is essential forhigh-quality code generation, while the SOTA models are becoming more robust toperturbations.    ",
    "date": "Submitted on 26 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14583",
    "pdf_link": "https://arxiv.org/pdf/2306.14583"
  },
  {
    "title": "Title:Fauno: The Italian Large Language Model that will leave you senza parole!",
    "authors": "Authors:Andrea Bacciu, Giovanni Trappolini, Andrea Santilli, Emanuele Rodol\u00e0, Fabrizio Silvestri",
    "abstract": "Abstract:  This paper presents Fauno, the first and largest open-source Italianconversational Large Language Model (LLM). Our goal with Fauno is todemocratize the study of LLMs in Italian, demonstrating that obtaining afine-tuned conversational bot with a single GPU is possible. In addition, werelease a collection of datasets for conversational AI in Italian. The datasetson which we fine-tuned Fauno include various topics such as general questionanswering, computer science, and medical questions. We release our code anddatasets on \\url{this https URL}    ",
    "date": "Submitted on 26 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14457",
    "pdf_link": "https://arxiv.org/pdf/2306.14457"
  },
  {
    "title": "Title:MotionGPT: Human Motion as a Foreign Language",
    "authors": "Authors:Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, Tao Chen",
    "abstract": "Abstract:  Though the advancement of pre-trained large language models unfolds, theexploration of building a unified model for language and other multi-modaldata, such as motion, remains challenging and untouched so far. Fortunately,human motion displays a semantic coupling akin to human language, oftenperceived as a form of body language. By fusing language data with large-scalemotion models, motion-language pre-training that can enhance the performance ofmotion-related tasks becomes feasible. Driven by this insight, we proposeMotionGPT, a unified, versatile, and user-friendly motion-language model tohandle multiple motion-relevant tasks. Specifically, we employ the discretevector quantization for human motion and transfer 3D motion into motion tokens,similar to the generation process of word tokens. Building upon this \"motionvocabulary\", we perform language modeling on both motion and text in a unifiedmanner, treating human motion as a specific language. Moreover, inspired byprompt learning, we pre-train MotionGPT with a mixture of motion-language dataand fine-tune it on prompt-based question-and-answer tasks. Extensiveexperiments demonstrate that MotionGPT achieves state-of-the-art performanceson multiple motion tasks including text-driven motion generation, motioncaptioning, motion prediction, and motion in-between.    ",
    "date": "Submitted on 26 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14795",
    "pdf_link": "https://arxiv.org/pdf/2306.14795"
  },
  {
    "title": "Title:SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality",
    "authors": "Authors:Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, Ranjay Krishna",
    "abstract": "Abstract:  In the last year alone, a surge of new benchmarks to measure compositionalunderstanding of vision-language models have permeated the machine learningecosystem. Given an image, these benchmarks probe a model's ability to identifyits associated caption amongst a set of compositional distractors.Surprisingly, we find significant biases in all these benchmarks rendering themhackable. This hackability is so dire that blind models with no access to theimage outperform state-of-the-art vision-language models. To remedy thisrampant vulnerability, we introduce SugarCrepe, a new benchmark forvision-language compositionality evaluation. We employ large language models,instead of rule-based templates used in previous benchmarks, to generate fluentand sensical hard negatives, and utilize an adversarial refinement mechanism tomaximally reduce biases. We re-evaluate state-of-the-art models and recentlyproposed compositionality inducing strategies, and find that their improvementswere hugely overestimated, suggesting that more innovation is needed in thisimportant direction. We release SugarCrepe and the code for evaluation at:this https URL.    ",
    "date": "Submitted on 26 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14610",
    "pdf_link": "https://arxiv.org/pdf/2306.14610"
  },
  {
    "title": "Title:RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations",
    "authors": "Authors:Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, Dragomir Radev",
    "abstract": "Abstract:  Despite significant progress having been made in question answering ontabular data (Table QA), it's unclear whether, and to what extent existingTable QA models are robust to task-specific perturbations, e.g., replacing keyquestion entities or shuffling table columns. To systematically study therobustness of Table QA models, we propose a benchmark called RobuT, whichbuilds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) andincludes human-annotated adversarial perturbations in terms of table header,table content, and question. Our results indicate that both state-of-the-artTable QA models and large language models (e.g., GPT-3) with few-shot learningfalter in these adversarial sets. We propose to address this problem by usinglarge language models to generate adversarial examples to enhance training,which significantly improves the robustness of Table QA models. Our data andcode is publicly available at this https URL.    ",
    "date": "Submitted on 25 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14321",
    "pdf_link": "https://arxiv.org/pdf/2306.14321"
  },
  {
    "title": "Title:Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?",
    "authors": "Authors:Haohan Zhang, Fengrui Hua, Chengjin Xu, Jian Guo, Hao Kong, Ruiting Zuo",
    "abstract": "Abstract:  The rapid advancement of Large Language Models (LLMs) has led to extensivediscourse regarding their potential to boost the return of quantitative stocktrading strategies. This discourse primarily revolves around harnessing theremarkable comprehension capabilities of LLMs to extract sentiment factorswhich facilitate informed and high-frequency investment portfolio adjustments.To ensure successful implementations of these LLMs into the analysis of Chinesefinancial texts and the subsequent trading strategy development within theChinese stock market, we provide a rigorous and encompassing benchmark as wellas a standardized back-testing framework aiming at objectively assessing theefficacy of various types of LLMs in the specialized domain of sentiment factorextraction from Chinese news text data. To illustrate how our benchmark works,we reference three distinctive models: 1) the generative LLM (ChatGPT), 2) theChinese language-specific pre-trained LLM (Erlangshen-RoBERTa), and 3) thefinancial domain-specific fine-tuned LLM classifier(Chinese FinBERT). We applythem directly to the task of sentiment factor extraction from large volumes ofChinese news summary texts. We then proceed to building quantitative tradingstrategies and running back-tests under realistic trading scenarios based onthe derived sentiment factors and evaluate their performances with ourbenchmark. By constructing such a comparative analysis, we invoke the questionof what constitutes the most important element for improving a LLM'sperformance on extracting sentiment factors. And by ensuring that the LLMs areevaluated on the same benchmark, following the same standardized experimentalprocedures that are designed with sufficient expertise in quantitative trading,we make the first stride toward answering such a question.    ",
    "date": "Submitted on 25 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14222",
    "pdf_link": "https://arxiv.org/pdf/2306.14222"
  },
  {
    "title": "Title:Chain-of-Thought Prompt Distillation for Multimodal Named Entity and Multimodal Relation Extraction",
    "authors": "Authors:Feng Chen, Yujian Feng",
    "abstract": "Abstract:  Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction(MRE) necessitate the fundamental reasoning capacity for intricate linguisticand multimodal comprehension. In this study, we explore distilling thereasoning ability of large language models (LLMs) into a more compact studentmodel by generating a \\textit{chain of thought} (CoT) -- a sequence ofintermediate reasoning steps. Specifically, we commence by exemplifying theelicitation of such reasoning ability from LLMs through CoT prompts coveringmulti-grain (noun, sentence, multimodality) and data-augmentation (style,entity, image) dimensions. Subsequently, we present a novel conditional promptdistillation method to assimilate the commonsense reasoning ability from LLMs,thereby enhancing the utility of the student model in addressing text-onlyinputs without the requisite addition of image and CoT knowledge. Extensiveexperiments reveal that our approach attains state-of-the-art accuracy andmanifests a plethora of advantages concerning interpretability, dataefficiency, and cross-domain generalization on MNER and MRE datasets.    ",
    "date": "Submitted on 25 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14122",
    "pdf_link": "https://arxiv.org/pdf/2306.14122"
  },
  {
    "title": "Title:Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models",
    "authors": "Authors:Yinyu Lan, Yanru Wu, Wang Xu, Weiqiang Feng, Youhao Zhang",
    "abstract": "Abstract:  Entity-level fine-grained sentiment analysis in the financial domain is acrucial subtask of sentiment analysis and currently faces numerous challenges.The primary challenge stems from the lack of high-quality and large-scaleannotated corpora specifically designed for financial text sentiment analysis,which in turn limits the availability of data necessary for developingeffective text processing techniques. Recent advancements in large languagemodels (LLMs) have yielded remarkable performance in natural languageprocessing tasks, primarily centered around language pattern matching. In thispaper, we propose a novel and extensive Chinese fine-grained financialsentiment analysis dataset, FinChina SA, for enterprise early warning. Wethoroughly evaluate and experiment with well-known existing open-source LLMsusing our dataset. We firmly believe that our dataset will serve as a valuableresource to advance the exploration of real-world financial sentiment analysistasks, which should be the focus of future research. Our dataset and all codeto replicate the experimental results will be released.    ",
    "date": "Submitted on 25 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14096",
    "pdf_link": "https://arxiv.org/pdf/2306.14096"
  },
  {
    "title": "Title:Large Language Models as Sous Chefs: Revising Recipes with GPT-3",
    "authors": "Authors:Alyssa Hwang, Bryan Li, Zhaoyi Hou, Dan Roth",
    "abstract": "Abstract:  With their remarkably improved text generation and prompting capabilities,large language models can adapt existing written information into forms thatare easier to use and understand. In our work, we focus on recipes as anexample of complex, diverse, and widely used instructions. We develop a promptgrounded in the original recipe and ingredients list that breaks recipes downinto simpler steps. We apply this prompt to recipes from various worldcuisines, and experiment with several large language models (LLMs), findingbest results with GPT-3.5. We also contribute an Amazon Mechanical Turk taskthat is carefully designed to reduce fatigue while collecting human judgment ofthe quality of recipe revisions. We find that annotators usually prefer therevision over the original, demonstrating a promising application of LLMs inserving as digital sous chefs for recipes and beyond. We release our prompt,code, and MTurk template for public use.    ",
    "date": "Submitted on 24 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13986",
    "pdf_link": "https://arxiv.org/pdf/2306.13986"
  },
  {
    "title": "Title:IERL: Interpretable Ensemble Representation Learning -- Combining CrowdSourced Knowledge and Distributed Semantic Representations",
    "authors": "Authors:Yuxin Zi, Kaushik Roy, Vignesh Narayanan, Manas Gaur, Amit Sheth",
    "abstract": "Abstract:  Large Language Models (LLMs) encode meanings of words in the form ofdistributed semantics. Distributed semantics capture common statisticalpatterns among language tokens (words, phrases, and sentences) from largeamounts of data. LLMs perform exceedingly well across General LanguageUnderstanding Evaluation (GLUE) tasks designed to test a model's understandingof the meanings of the input tokens. However, recent studies have shown thatLLMs tend to generate unintended, inconsistent, or wrong texts as outputs whenprocessing inputs that were seen rarely during training, or inputs that areassociated with diverse contexts (e.g., well-known hallucination phenomenon inlanguage generation tasks). Crowdsourced and expert-curated knowledge graphssuch as ConceptNet are designed to capture the meaning of words from a compactset of well-defined contexts. Thus LLMs may benefit from leveraging suchknowledge contexts to reduce inconsistencies in outputs. We propose a novelensemble learning method, Interpretable Ensemble Representation Learning(IERL), that systematically combines LLM and crowdsourced knowledgerepresentations of input tokens. IERL has the distinct advantage of beinginterpretable by design (when was the LLM context used vs. when was theknowledge context used?) over state-of-the-art (SOTA) methods, allowingscrutiny of the inputs in conjunction with the parameters of the model,facilitating the analysis of models' inconsistent or irrelevant outputs.Although IERL is agnostic to the choice of LLM and crowdsourced knowledge, wedemonstrate our approach using BERT and ConceptNet. We report improved orcompetitive results with IERL across GLUE tasks over current SOTA methods andsignificantly enhanced model interpretability.    ",
    "date": "Submitted on 24 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13865",
    "pdf_link": "https://arxiv.org/pdf/2306.13865"
  },
  {
    "title": "Title:Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data",
    "authors": "Authors:Alycia Lee, Brando Miranda, Sanmi Koyejo",
    "abstract": "Abstract:  Current trends to pre-train capable Large Language Models (LLMs) mostly focuson scaling of model and dataset size. However, the quality of pre-training datais an important factor for training powerful LLMs, yet it is a nebulous conceptthat has not been fully characterized. Therefore, we use the recently proposedTask2Vec diversity coefficient to ground and understand formal aspects of dataquality, to go beyond scale alone. Specifically, we measure the diversitycoefficient of publicly available pre-training datasets to demonstrate thattheir formal diversity is high when compared to theoretical lower and upperbounds. In addition, to build confidence in the diversity coefficient, weconduct interpretability experiments and find that the coefficient aligns withintuitive properties of diversity, e.g., it increases as the number of latentconcepts increases. We conclude the diversity coefficient is reliable, showit's high for publicly available LLM datasets, and conjecture it can be used tobuild useful diverse datasets for LLMs.    ",
    "date": "Submitted on 24 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13840",
    "pdf_link": "https://arxiv.org/pdf/2306.13840"
  },
  {
    "title": "Title:DesCo: Learning Object Recognition with Rich Language Descriptions",
    "authors": "Authors:Liunian Harold Li, Zi-Yi Dou, Nanyun Peng, Kai-Wei Chang",
    "abstract": "Abstract:  Recent development in vision-language approaches has instigated a paradigmshift in learning visual recognition models from language supervision. Theseapproaches align objects with language queries (e.g. \"a photo of a cat\") andimprove the models' adaptability to identify novel objects and domains.Recently, several studies have attempted to query these models with complexlanguage expressions that include specifications of fine-grained semanticdetails, such as attributes, shapes, textures, and relations. However, simplyincorporating language descriptions as queries does not guarantee accurateinterpretation by the models. In fact, our experiments show that GLIP, thestate-of-the-art vision-language model for object detection, often disregardscontextual information in the language descriptions and instead relies heavilyon detecting objects solely by their names. To tackle the challenges, wepropose a new description-conditioned (DesCo) paradigm of learning objectrecognition models with rich language descriptions consisting of two majorinnovations: 1) we employ a large language model as a commonsense knowledgeengine to generate rich language descriptions of objects based on object namesand the raw image-text caption; 2) we design context-sensitive queries toimprove the model's ability in deciphering intricate nuances embedded withindescriptions and enforce the model to focus on context rather than object namesalone. On two novel object detection benchmarks, LVIS and OminiLabel, under thezero-shot detection setting, our approach achieves 34.8 APr minival (+9.1) and29.3 AP (+3.6), respectively, surpassing the prior state-of-the-art models,GLIP and FIBER, by a large margin.    ",
    "date": "Submitted on 24 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14060",
    "pdf_link": "https://arxiv.org/pdf/2306.14060"
  },
  {
    "title": "Title:LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding",
    "authors": "Authors:Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, Annice Kim",
    "abstract": "Abstract:  Deductive coding is a widely used qualitative research method for determiningthe prevalence of themes across documents. While useful, deductive coding isoften burdensome and time consuming since it requires researchers to read,interpret, and reliably categorize a large body of unstructured text documents.Large language models (LLMs), like ChatGPT, are a class of quickly evolving AItools that can perform a range of natural language processing and reasoningtasks. In this study, we explore the use of LLMs to reduce the time it takesfor deductive coding while retaining the flexibility of a traditional contentanalysis. We outline the proposed approach, called LLM-assisted contentanalysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on apublicly available deductive coding data set. Additionally, we conduct anempirical benchmark using LACA on 4 publicly available data sets to assess thebroader question of how well GPT-3.5 performs across a range of deductivecoding tasks. Overall, we find that GPT-3.5 can often perform deductive codingat levels of agreement comparable to human coders. Additionally, we demonstratethat LACA can help refine prompts for deductive coding, identify codes forwhich an LLM is randomly guessing, and help assess when to use LLMs vs. humancoders for deductive coding. We conclude with several implications for futurepractice of deductive coding and related research methods.    ",
    "date": "Submitted on 23 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14924",
    "pdf_link": "https://arxiv.org/pdf/2306.14924"
  },
  {
    "title": "Title:Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models",
    "authors": "Authors:Adel Elmahdy, Ahmed Salem",
    "abstract": "Abstract:  Natural language processing (NLP) models have become increasingly popular inreal-world applications, such as text classification. However, they arevulnerable to privacy attacks, including data reconstruction attacks that aimto extract the data used to train the model. Most previous studies on datareconstruction attacks have focused on LLM, while classification models wereassumed to be more secure. In this work, we propose a new targeted datareconstruction attack called the Mix And Match attack, which takes advantage ofthe fact that most classification models are based on LLM. The Mix And Matchattack uses the base model of the target model to generate candidate tokens andthen prunes them using the classification head. We extensively demonstrate theeffectiveness of the attack using both random and organic canaries. This workhighlights the importance of considering the privacy risks associated with datareconstruction attacks in classification models and offers insights intopossible leakages.    ",
    "date": "Submitted on 23 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13789",
    "pdf_link": "https://arxiv.org/pdf/2306.13789"
  },
  {
    "title": "Title:Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
    "authors": "Authors:Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein",
    "abstract": "Abstract:  With the rise of Large Language Models (LLMs) and their ubiquitous deploymentin diverse domains, measuring language model behavior on realistic data isimperative. For example, a company deploying a client-facing chatbot mustensure that the model will not respond to client requests with profanity.Current evaluations approach this problem using small, domain-specific datasetswith human-curated labels. These evaluation sets are often sampled from anarrow and simplified distribution, and data sources can unknowingly be leakedinto the training set which can lead to misleading evaluations. To bypass thesedrawbacks, we propose a framework for self-supervised evaluation of LLMs byanalyzing their sensitivity or invariance to transformations on the input text.Self-supervised evaluation can directly monitor LLM behavior on datasetscollected in the wild or streamed during live model deployment. We demonstrateself-supervised evaluation strategies for measuring closed-book knowledge,toxicity, and long-range context dependence, in addition to sensitivity togrammatical structure and tokenization errors. When comparisons to similarhuman-labeled benchmarks are available, we find strong correlations betweenself-supervised and human-supervised evaluations. The self-supervised paradigmcomplements current evaluation strategies that rely on labeled data.    ",
    "date": "Submitted on 23 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13651",
    "pdf_link": "https://arxiv.org/pdf/2306.13651"
  },
  {
    "title": "Title:ToolQA: A Dataset for LLM Question Answering with External Tools",
    "authors": "Authors:Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang",
    "abstract": "Abstract:  Large Language Models (LLMs) have demonstrated impressive performance invarious NLP tasks, but they still suffer from challenges such as hallucinationand weak numerical reasoning. To overcome these challenges, external tools canbe used to enhance LLMs' question-answering abilities. However, currentevaluation methods do not distinguish between questions that can be answeredusing LLMs' internal knowledge and those that require external informationthrough tool use. To address this issue, we introduce a new dataset calledToolQA, which is designed to faithfully evaluate LLMs' ability to use externaltools for question answering. Our development of ToolQA involved a scalable,automated process for dataset curation, along with 13 specialized toolsdesigned for interaction with external knowledge in order to answer questions.Importantly, we strive to minimize the overlap between our benchmark data andLLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-usereasoning abilities. We conducted an in-depth diagnosis of existing tool-useLLMs to highlight their strengths, weaknesses, and potential improvements. Ourfindings set a new benchmark for evaluating LLMs and suggest new directions forfuture advancements. Our data and code are freely available to the broaderscientific community on GitHub.    ",
    "date": "Submitted on 23 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13304",
    "pdf_link": "https://arxiv.org/pdf/2306.13304"
  },
  {
    "title": "Title:GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models",
    "authors": "Authors:Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, Olivier Bachem",
    "abstract": "Abstract:  Knowledge distillation is commonly used for compressing neural networks toreduce their inference cost and memory footprint. However, current distillationmethods for auto-regressive models, such as generative language models (LMs),suffer from two key issues: (1) distribution mismatch between output sequencesduring training and the sequences generated by the student during itsdeployment, and (2) model under-specification, where the student model may notbe expressive enough to fit the teacher's distribution. To address theseissues, we propose Generalized Knowledge Distillation (GKD). GKD mitigatesdistribution mismatch by sampling output sequences from the student duringtraining. Furthermore, GKD handles model under-specification by optimizingalternative divergences, such as reverse KL, that focus on generating samplesfrom the student that are likely under the teacher's distribution. Wedemonstrate that GKD outperforms commonly-used approaches for distilling LLMson summarization, machine translation, and arithmetic reasoning tasks.    ",
    "date": "Submitted on 23 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13649",
    "pdf_link": "https://arxiv.org/pdf/2306.13649"
  },
  {
    "title": "Title:Max-Margin Token Selection in Attention Mechanism",
    "authors": "Authors:Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, Samet Oymak",
    "abstract": "Abstract:  Attention mechanism is a central component of the transformer architecturewhich led to the phenomenal success of large language models. However, thetheoretical principles underlying the attention mechanism are poorlyunderstood, especially its nonconvex optimization dynamics. In this work, weexplore the seminal softmax-attention model $f(\\boldsymbol{X})=\\langle\\boldsymbol{Xv}, \\texttt{softmax}(\\boldsymbol{XWp})\\rangle$, where$\\boldsymbol{X}$ is the token sequence and$(\\boldsymbol{v},\\boldsymbol{W},\\boldsymbol{p})$ are trainable parameters. Weprove that running gradient descent on $\\boldsymbol{p}$, or equivalently$\\boldsymbol{W}$, converges in direction to a max-margin solution thatseparates $\\textit{locally-optimal}$ tokens from non-optimal ones. This clearlyformalizes attention as an optimal token selection mechanism. Remarkably, ourresults are applicable to general data and precisely characterize$\\textit{optimality}$ of tokens in terms of the value embeddings$\\boldsymbol{Xv}$ and problem geometry. We also provide a broaderregularization path analysis that establishes the margin maximizing nature ofattention even for nonlinear prediction heads. When optimizing $\\boldsymbol{v}$and $\\boldsymbol{p}$ simultaneously with logistic loss, we identify conditionsunder which the regularization paths directionally converge to their respectivehard-margin SVM solutions where $\\boldsymbol{v}$ separates the input featuresbased on their labels. Interestingly, the SVM formulation of $\\boldsymbol{p}$is influenced by the support vector geometry of $\\boldsymbol{v}$. Finally, weverify our theoretical findings via numerical experiments and provide insights.    ",
    "date": "Submitted on 23 Jun 2023 (v1), last revised 27 Jun 2023 (this version, v2)",
    "abstract_link": "https://arxiv.org/abs/2306.13596",
    "pdf_link": "https://arxiv.org/pdf/2306.13596"
  },
  {
    "title": "Title:A Survey on Multimodal Large Language Models",
    "authors": "Authors:Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen",
    "abstract": "Abstract:  Multimodal Large Language Model (MLLM) recently has been a new risingresearch hotspot, which uses powerful Large Language Models (LLMs) as a brainto perform multimodal tasks. The surprising emergent capabilities of MLLM, suchas writing stories based on images and OCR-free math reasoning, are rare intraditional methods, suggesting a potential path to artificial generalintelligence. In this paper, we aim to trace and summarize the recent progressof MLLM. First of all, we present the formulation of MLLM and delineate itsrelated concepts. Then, we discuss the key techniques and applications,including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning(M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning(LAVR). Finally, we discuss existing challenges and point out promisingresearch directions. In light of the fact that the era of MLLM has only justbegun, we will keep updating this survey and hope it can inspire more research.An associated GitHub link collecting the latest papers is available atthis https URL.    ",
    "date": "Submitted on 23 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13549",
    "pdf_link": "https://arxiv.org/pdf/2306.13549"
  },
  {
    "title": "Title:DiversiGATE: A Comprehensive Framework for Reliable Large Language Models",
    "authors": "Authors:Shima Imani, Ali Beyram, Harsh Shrivastava",
    "abstract": "Abstract:  In this paper, we introduce DiversiGATE, a unified framework thatconsolidates diverse methodologies for LLM verification. The proposed frameworkcomprises two main components: Diversification and Aggregation which provide aholistic perspective on existing verification approaches, such asSelf-Consistency, Math Prompter and WebGPT. Furthermore, we propose a novel`SelfLearner' model that conforms to the DiversiGATE framework which can learnfrom its own outputs and refine its performance over time, leading to improvedaccuracy. To evaluate the effectiveness of SelfLearner, we conducted a rigorousseries of experiments, including tests on synthetic data as well as on populararithmetic reasoning benchmarks such as GSM8K. Our results demonstrate that ourapproach outperforms traditional LLMs, achieving a considerable 54.8% -> 61.8%improvement on the GSM8K benchmark.    ",
    "date": "Submitted on 22 Jun 2023 (v1), last revised 26 Jun 2023 (this version, v2)",
    "abstract_link": "https://arxiv.org/abs/2306.13230",
    "pdf_link": "https://arxiv.org/pdf/2306.13230"
  },
  {
    "title": "Title:Visual Adversarial Examples Jailbreak Large Language Models",
    "authors": "Authors:Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, Prateek Mittal",
    "abstract": "Abstract:  Recently, there has been a surge of interest in introducing vision into LargeLanguage Models (LLMs). The proliferation of large Visual Language Models(VLMs), such as Flamingo, BLIP-2, and GPT-4, signifies an exciting convergenceof advancements in both visual and language foundation models. Yet, the risksassociated with this integrative approach are largely unexamined. In thispaper, we shed light on the security and safety implications of this trend.First, we underscore that the continuous and high-dimensional nature of theadditional visual input space intrinsically makes it a fertile ground foradversarial attacks. This unavoidably expands the attack surfaces of LLMs.Second, we highlight that the broad functionality of LLMs also presents visualattackers with a wider array of achievable adversarial objectives, extendingthe implications of security failures beyond mere misclassification. Toelucidate these risks, we study adversarial examples in the visual input spaceof a VLM. Specifically, against MiniGPT-4, which incorporates safety mechanismsthat can refuse harmful instructions, we present visual adversarial examplesthat can circumvent the safety mechanisms and provoke harmful behaviors of themodel. Remarkably, we discover that adversarial examples, even if optimized ona narrow, manually curated derogatory corpus against specific social groups,can universally jailbreak the model's safety mechanisms. A single suchadversarial example can generally undermine MiniGPT-4's safety, enabling it toheed a wide range of harmful instructions and produce harmful content farbeyond simply imitating the derogatory corpus used in optimization. Unveilingthese risks, we accentuate the urgent need for comprehensive risk assessments,robust defense strategies, and the implementation of responsible practices forthe secure and safe utilization of VLMs.    ",
    "date": "Submitted on 22 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13213",
    "pdf_link": "https://arxiv.org/pdf/2306.13213"
  },
  {
    "title": "Title:Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
    "authors": "Authors:Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi",
    "abstract": "Abstract:  The task of empowering large language models (LLMs) to accurately expresstheir confidence, referred to as confidence elicitation, is essential inensuring reliable and trustworthy decision-making processes. Previous methods,which primarily rely on model logits, have become less suitable for LLMs andeven infeasible with the rise of closed-source LLMs (e.g., commercialized LLMAPIs). This leads to a growing need to explore the untapped area of\\emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence,in this study, we investigate approaches for confidence elicitation that do notrequire model fine-tuning or access to proprietary information. We introducethree categories of methods: verbalize-based, consistency-based, and theirhybrid methods for benchmarking, and evaluate their performance across fivetypes of datasets and four widely-used LLMs. Our analysis of these methodsuncovers several key insights: 1) LLMs often exhibit a high degree ofoverconfidence when verbalizing their confidence; 2) Prompting strategies suchas CoT, Top-K and Multi-step confidences improve calibration of verbalizedconfidence; 3) Consistency-based methods outperform the verbalized confidencesin most cases, with particularly notable improvements on the arithmeticreasoning task; 4) Hybrid methods consistently deliver the best performanceover their baselines, thereby emerging as a promising state-of-the-artapproach; 5) Despite these advancements, all investigated methods continue tostruggle with challenging tasks, such as those requiring professionalknowledge, leaving significant scope for improvement of confidence elicitation.    ",
    "date": "Submitted on 22 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13063",
    "pdf_link": "https://arxiv.org/pdf/2306.13063"
  },
  {
    "title": "Title:Towards Explainable Evaluation Metrics for Machine Translation",
    "authors": "Authors:Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, Steffen Eger",
    "abstract": "Abstract:  Unlike classical lexical overlap metrics such as BLEU, most currentevaluation metrics for machine translation (for example, COMET or BERTScore)are based on black-box large language models. They often achieve strongcorrelations with human judgments, but recent research indicates that thelower-quality classical metrics remain dominant, one of the potential reasonsbeing that their decision processes are more transparent. To foster morewidespread acceptance of novel high-quality metrics, explainability thusbecomes crucial. In this concept paper, we identify key properties as well askey goals of explainable machine translation metrics and provide acomprehensive synthesis of recent techniques, relating them to our establishedgoals and properties. In this context, we also discuss the lateststate-of-the-art approaches to explainable metrics based on generative modelssuch as ChatGPT and GPT4. Finally, we contribute a vision of next-generationapproaches, including natural language explanations. We hope that our work canhelp catalyze and guide future research on explainable evaluation metrics and,mediately, also contribute to better and more transparent machine translationsystems.    ",
    "date": "Submitted on 22 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13041",
    "pdf_link": "https://arxiv.org/pdf/2306.13041"
  },
  {
    "title": "Title:Tracking public attitudes toward ChatGPT on Twitter using sentiment analysis and topic modeling",
    "authors": "Authors:Ratanond Koonchanok, Yanling Pan, Hyeju Jang",
    "abstract": "Abstract:  ChatGPT sets a new record with the fastest-growing user base, as a chatbotpowered by a large language model (LLM). While it demonstrates state-of-the-artcapabilities in a variety of language-generating tasks, it also raiseswidespread public concerns regarding its societal impact. In this paper, weutilize natural language processing approaches to investigate the publicattitudes towards ChatGPT by applying sentiment analysis and topic modelingtechniques to Twitter data. Our result shows that the overall sentiment islargely neutral to positive, which also holds true across different occupationgroups. Among a wide range of topics mentioned in tweets, the most populartopics are Artificial Intelligence, Search Engines, Education, Writing, andQuestion Answering.    ",
    "date": "Submitted on 22 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.12951",
    "pdf_link": "https://arxiv.org/pdf/2306.12951"
  },
  {
    "title": "Title:AudioPaLM: A Large Language Model That Can Speak and Listen",
    "authors": "Authors:Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zal\u00e1n Borsos, F\u00e9lix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimirovi\u0107, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, Christian Frank",
    "abstract": "Abstract:  We introduce AudioPaLM, a large language model for speech understanding andgeneration. AudioPaLM fuses text-based and speech-based language models, PaLM-2[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unifiedmultimodal architecture that can process and generate text and speech withapplications including speech recognition and speech-to-speech translation.AudioPaLM inherits the capability to preserve paralinguistic information suchas speaker identity and intonation from AudioLM and the linguistic knowledgepresent only in text large language models such as PaLM-2. We demonstrate thatinitializing AudioPaLM with the weights of a text-only large language modelimproves speech processing, successfully leveraging the larger quantity of texttraining data used in pretraining to assist with the speech tasks. Theresulting model significantly outperforms existing systems for speechtranslation tasks and has the ability to perform zero-shot speech-to-texttranslation for many languages for which input/target language combinationswere not seen in training. AudioPaLM also demonstrates features of audiolanguage models, such as transferring a voice across languages based on a shortspoken prompt. We release examples of our method atthis https URL",
    "date": "Submitted on 22 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.12925",
    "pdf_link": "https://arxiv.org/pdf/2306.12925"
  },
  {
    "title": "Title:Generative Multimodal Entity Linking",
    "authors": "Authors:Senbao Shi, Zhenran Xu, Baotian Hu, Min Zhang",
    "abstract": "Abstract:  Multimodal Entity Linking (MEL) is the task of mapping mentions withmultimodal contexts to the referent entities from a knowledge base (e.g.,Wikipedia). Prior MEL methods mainly focus on designing complex multimodalinteraction mechanisms and require fine-tuning all model parameters, which canbe prohibitively costly and difficult to scale in the era of Large LanguageModels (LLMs). In this work, we propose GEMEL, a simple yet effectiveGenerative Multimodal Entity Linking method, which leverages the capabilitiesof LLMs from large-scale pre-training to directly generate target entity names.We keep the vision and language model frozen and only train a linear layer toenable cross-modality interactions. To adapt LLMs to the MEL task, we takeadvantage of the emerging in-context learning (ICL) capability of LLMs byretrieving multimodal instances as demonstrations. Extensive experiments showthat with only ~0.3% of the model parameters fine-tuned, GEMEL achievesstate-of-the-art results on two well-established MEL datasets (4.1% accuracygains on WikiDiverse and 15.4% accuracy gains on WikiMEL). Our approach iscompatible with any off-the-shelf language model, paving the way towards anefficient and general solution for utilizing LLMs in the MEL task.    ",
    "date": "Submitted on 22 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.12725",
    "pdf_link": "https://arxiv.org/pdf/2306.12725"
  },
  {
    "title": "Title:From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought",
    "authors": "Authors:Lionel Wong, Gabriel Grand, Alexander K. Lew, Noah D. Goodman, Vikash K. Mansinghka, Jacob Andreas, Joshua B. Tenenbaum",
    "abstract": "Abstract:  How does language inform our downstream thinking? In particular, how dohumans make meaning from language--and how can we leverage a theory oflinguistic meaning to build machines that think in more human-like ways? Inthis paper, we propose rational meaning construction, a computational frameworkfor language-informed thinking that combines neural language models withprobabilistic models for rational inference. We frame linguistic meaning as acontext-sensitive mapping from natural language into a probabilistic languageof thought (PLoT)--a general-purpose symbolic substrate for generative worldmodeling. Our architecture integrates two computational tools that have notpreviously come together: we model thinking with probabilistic programs, anexpressive representation for commonsense reasoning; and we model meaningconstruction with large language models (LLMs), which support broad-coveragetranslation from natural language utterances to code expressions in aprobabilistic programming language. We illustrate our framework throughexamples covering four core domains from cognitive science: probabilisticreasoning, logical and relational reasoning, visual and physical reasoning, andsocial reasoning. In each, we show that LLMs can generate context-sensitivetranslations that capture pragmatically-appropriate linguistic meanings, whileBayesian inference with the generated programs supports coherent and robustcommonsense reasoning. We extend our framework to integratecognitively-motivated symbolic modules (physics simulators, graphics engines,and planning algorithms) to provide a unified commonsense thinking interfacefrom language. Finally, we explore how language can drive the construction ofworld models themselves. We hope this work will provide a roadmap towardscognitive models and AI systems that synthesize the insights of both modern andclassical computational perspectives.    ",
    "date": "Submitted on 22 Jun 2023 (v1), last revised 23 Jun 2023 (this version, v2)",
    "abstract_link": "https://arxiv.org/abs/2306.12672",
    "pdf_link": "https://arxiv.org/pdf/2306.12672"
  },
  {
    "title": "Title:Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models",
    "authors": "Authors:Boyu Zhang, Hongyang Yang, Xiao-Yang Liu",
    "abstract": "Abstract:  Sentiment analysis is a vital tool for uncovering insights from financialarticles, news, and social media, shaping our understanding of marketmovements. Despite the impressive capabilities of large language models (LLMs)in financial natural language processing (NLP), they still struggle withaccurately interpreting numerical values and grasping financial context,limiting their effectiveness in predicting financial sentiment. In this paper,we introduce a simple yet effective instruction tuning approach to addressthese issues. By transforming a small portion of supervised financial sentimentanalysis data into instruction data and fine-tuning a general-purpose LLM withthis method, we achieve remarkable advancements in financial sentimentanalysis. In the experiment, our approach outperforms state-of-the-artsupervised sentiment analysis models, as well as widely used LLMs like ChatGPTand LLaMAs, particularly in scenarios where numerical understanding andcontextual comprehension are vital.    ",
    "date": "Submitted on 22 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.12659",
    "pdf_link": "https://arxiv.org/pdf/2306.12659"
  },
  {
    "title": "Title:Identifying and Extracting Rare Disease Phenotypes with Large Language Models",
    "authors": "Authors:Cathy Shyr, Yan Hu, Paul A. Harris, Hua Xu",
    "abstract": "Abstract:  Rare diseases (RDs) are collectively common and affect 300 million peopleworldwide. Accurate phenotyping is critical for informing diagnosis andtreatment, but RD phenotypes are often embedded in unstructured text andtime-consuming to extract manually. While natural language processing (NLP)models can perform named entity recognition (NER) to automate extraction, amajor bottleneck is the development of a large, annotated corpus for modeltraining. Recently, prompt learning emerged as an NLP paradigm that can lead tomore generalizable results without any (zero-shot) or few labeled samples(few-shot). Despite growing interest in ChatGPT, a revolutionary large languagemodel capable of following complex human prompts and generating high-qualityresponses, none have studied its NER performance for RDs in the zero- andfew-shot settings. To this end, we engineered novel prompts aimed at extractingRD phenotypes and, to the best of our knowledge, are the first the establish abenchmark for evaluating ChatGPT's performance in these settings. We comparedits performance to the traditional fine-tuning approach and conducted anin-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted inhigher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in thezero- and few-shot settings, respectively). Despite this, ChatGPT achievedsimilar or higher accuracy for certain entities (i.e., rare diseases and signs)in the one-shot setting (F1 of 0.776 and 0.725). This suggests that withappropriate prompt engineering, ChatGPT has the potential to match oroutperform fine-tuned language models for certain entity types with just onelabeled sample. While the proliferation of large language models may provideopportunities for supporting RD diagnosis and treatment, researchers andclinicians should critically evaluate model outputs and be well-informed oftheir limitations.    ",
    "date": "Submitted on 22 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.12656",
    "pdf_link": "https://arxiv.org/pdf/2306.12656"
  },
  {
    "title": "Title:Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US",
    "authors": "Authors:Jonathan H. Rystr\u00f8m",
    "abstract": "Abstract:  As generative language models are deployed in ever-wider contexts, concernsabout their political values have come to the forefront with critique from allparts of the political spectrum that the models are biased and lack neutrality.However, the question of what neutrality is and whether it is desirable remainsunderexplored. In this paper, I examine neutrality through an audit of Delphi[arXiv:2110.07574], a large language model designed for crowdsourced ethics. Ianalyse how Delphi responds to politically controversial questions compared todifferent US political subgroups. I find that Delphi is poorly calibrated withrespect to confidence and exhibits a significant political skew. Based on theseresults, I examine the question of neutrality from a data-feminist lens, interms of how notions of neutrality shift power and further marginalise unheardvoices. These findings can hopefully contribute to a more reflexive debateabout the normative questions of alignment and what role we want generativemodels to play in society.    ",
    "date": "Submitted on 22 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.13000",
    "pdf_link": "https://arxiv.org/pdf/2306.13000"
  },
  {
    "title": "Title:Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing",
    "authors": "Authors:Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort",
    "abstract": "Abstract:  Transformer models have been widely adopted in various domains over the lastyears, and especially large language models have advanced the field of AIsignificantly. Due to their size, the capability of these networks hasincreased tremendously, but this has come at the cost of a significant increasein necessary compute. Quantization is one of the most effective ways to reducethe computational time and memory consumption of neural networks. Many studieshave shown, however, that modern transformer models tend to learn strongoutliers in their activations, making them difficult to quantize. To retainacceptable performance, the existence of these outliers requires activations tobe in higher bitwidth or the use of different numeric formats, extrafine-tuning, or other workarounds. We show that strong outliers are related tovery specific behavior of attention heads that try to learn a \"no-op\" or just apartial update of the residual. To achieve the exact zeros needed in theattention matrix for a no-update, the input to the softmax is pushed to belarger and larger during training, causing outliers in other parts of thenetwork. Based on these observations, we propose two simple (independent)modifications to the attention mechanism - clipped softmax and gated attention.We empirically show that models pre-trained using our methods learnsignificantly smaller outliers while maintaining and sometimes even improvingthe floating-point task performance. This enables us to quantize transformersto full INT8 quantization of the activations without any additional effort. Wedemonstrate the effectiveness of our methods on both language models (BERT,OPT) and vision transformers.    ",
    "date": "Submitted on 22 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.12929",
    "pdf_link": "https://arxiv.org/pdf/2306.12929"
  },
  {
    "title": "Title:Testing of Detection Tools for AI-Generated Text",
    "authors": "Authors:Debora Weber-Wulff (University of Applied Sciences HTW Berlin, Germany), Alla Anohina-Naumeca (Riga Technical University, Latvia), Sonja Bjelobaba (Uppsala University, Sweden), Tom\u00e1\u0161 Folt\u00fdnek (Masaryk University, Czechia), Jean Guerrero-Dib (Universidad de Monterrey, Mexico), Olumide Popoola (Queen Mary's University, UK), Petr \u0160igut (Masaryk University, Czechia), Lorna Waddington (University of Leeds, UK)",
    "abstract": "Abstract:  Recent advances in generative pre-trained transformer large language modelshave emphasised the potential risks of unfair use of artificial intelligence(AI) generated content in an academic environment and intensified efforts insearching for solutions to detect such content. The paper examines the generalfunctionality of detection tools for artificial intelligence generated text andevaluates them based on accuracy and error type analysis. Specifically, thestudy seeks to answer research questions about whether existing detection toolscan reliably differentiate between human-written text and ChatGPT-generatedtext, and whether machine translation and content obfuscation techniques affectthe detection of AIgenerated text. The research covers 12 publicly availabletools and two commercial systems (Turnitin and PlagiarismCheck) that are widelyused in the academic setting. The researchers conclude that the availabledetection tools are neither accurate nor reliable and have a main bias towardsclassifying the output as human-written rather than detecting AIgenerated text.Furthermore, content obfuscation techniques significantly worsen theperformance of tools. The study makes several significant contributions. First,it summarises up-to-date similar scientific and non-scientific efforts in thefield. Second, it presents the result of one of the most comprehensive testsconducted so far, based on a rigorous research methodology, an originaldocument set, and a broad coverage of tools. Third, it discusses theimplications and drawbacks of using detection tools for AI-generated text inacademic settings.    ",
    "date": "Submitted on 21 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.15666",
    "pdf_link": "https://arxiv.org/pdf/2306.15666"
  },
  {
    "title": "Title:Understanding Social Reasoning in Language Models with Language Models",
    "authors": "Authors:Kanishk Gandhi, Jan-Philipp Fr\u00e4nken, Tobias Gerstenberg, Noah D. Goodman",
    "abstract": "Abstract:  As Large Language Models (LLMs) become increasingly integrated into oureveryday lives, understanding their ability to comprehend human mental statesbecomes critical for ensuring effective interactions. However, despite therecent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities ofLLMs, the degree to which these models can align with human ToM remains anuanced topic of exploration. This is primarily due to two distinct challenges:(1) the presence of inconsistent results from previous evaluations, and (2)concerns surrounding the validity of existing evaluation methodologies. Toaddress these challenges, we present a novel framework for procedurallygenerating evaluations with LLMs by populating causal templates. Using ourframework, we create a new social reasoning benchmark (BigToM) for LLMs whichconsists of 25 controls and 5,000 model-written evaluations. We find that humanparticipants rate the quality of our benchmark higher than previouscrowd-sourced evaluations and comparable to expert-written evaluations. UsingBigToM, we evaluate the social reasoning capabilities of a variety of LLMs andcompare model performances with human performance. Our results suggest thatGPT4 has ToM capabilities that mirror human inference patterns, though lessreliable, while other LLMs struggle.    ",
    "date": "Submitted on 21 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.15448",
    "pdf_link": "https://arxiv.org/pdf/2306.15448"
  },
  {
    "title": "Title:ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews",
    "authors": "Authors:Mike D'Arcy, Alexis Ross, Erin Bransom, Bailey Kuehl, Jonathan Bragg, Tom Hope, Doug Downey",
    "abstract": "Abstract:  Revising scientific papers based on peer feedback is a challenging task thatrequires not only deep scientific knowledge and reasoning, but also the abilityto recognize the implicit requests in high-level feedback and to choose thebest of many possible ways to update the manuscript in response. We introducethis task for large language models and release ARIES, a dataset of reviewcomments and their corresponding paper edits, to enable training and evaluatingmodels. We study two versions of the task: comment-edit alignment and editgeneration, and evaluate several baselines, including GPT-4. We find thatmodels struggle even to identify the edits that correspond to a comment,especially in cases where the comment is phrased in an indirect way or wherethe edit addresses the spirit of a comment but not the precise request. Whentasked with generating edits, GPT-4 often succeeds in addressing comments on asurface level, but it rigidly follows the wording of the feedback rather thanthe underlying intent, and includes fewer technical details than human-writtenedits. We hope that our formalization, dataset, and analysis will form afoundation for future work in this area.    ",
    "date": "Submitted on 21 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.12587",
    "pdf_link": "https://arxiv.org/pdf/2306.12587"
  },
  {
    "title": "Title:Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases",
    "authors": "Authors:Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada",
    "abstract": "Abstract:  This paper investigates whether current large language models exhibit biasesin logical reasoning, similar to humans. Specifically, we focus on syllogisticreasoning, a well-studied form of inference in the cognitive science of humandeduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO,originally designed for psychological experiments that assess human logicalabilities in syllogistic reasoning. The dataset consists of syllogisticinferences in both English and Japanese. We examine three types of biasesobserved in human syllogistic reasoning: belief biases, conversion errors, andatmosphere effects. Our findings demonstrate that current large language modelsstruggle more with problems involving these three types of biases.    ",
    "date": "Submitted on 21 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.12567",
    "pdf_link": "https://arxiv.org/pdf/2306.12567"
  },
  {
    "title": "Title:SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning",
    "authors": "Authors:Yunxiang Zhang, Xiaojun Wan",
    "abstract": "Abstract:  Recently, commonsense reasoning in text generation has attracted muchattention. Generative commonsense reasoning is the task that requires machines,given a group of keywords, to compose a single coherent sentence withcommonsense plausibility. While existing datasets targeting generativecommonsense reasoning focus on everyday scenarios, it is unclear how wellmachines reason under specific geographical and temporal contexts. We formalizethis challenging task as SituatedGen, where machines with commonsense shouldgenerate a pair of contrastive sentences given a group of keywords includinggeographical or temporal entities. We introduce a corresponding English datasetconsisting of 8,268 contrastive sentence pairs, which are built upon severalexisting commonsense reasoning benchmarks with minimal manual labor.Experiments show that state-of-the-art generative language models struggle togenerate sentences with commonsense plausibility and still lag far behind humanperformance. Our dataset is publicly available atthis https URL.    ",
    "date": "Submitted on 21 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.12552",
    "pdf_link": "https://arxiv.org/pdf/2306.12552"
  },
  {
    "title": "Title:Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference",
    "authors": "Authors:Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, Nicolas Le Roux",
    "abstract": "Abstract:  We view large language models (LLMs) as stochastic \\emph{language layers} ina network, where the learnable parameters are the natural language\\emph{prompts} at each layer. We stack two such layers, feeding the output ofone layer to the next. We call the stacked architecture a \\emph{Deep LanguageNetwork} (DLN). We first show how to effectively perform prompt optimizationfor a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs(DLN-2), where two prompts must be learnt. We consider the output of the firstlayer as a latent variable to marginalize, and devise a variational inferencealgorithm for joint prompt training. A DLN-2 reaches higher performance than asingle layer, sometimes comparable to few-shot GPT-4 even when each LLM in thenetwork is smaller and less powerful. The DLN code is open source:this https URL .    ",
    "date": "Submitted on 21 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.12509",
    "pdf_link": "https://arxiv.org/pdf/2306.12509"
  },
  {
    "title": "Title:Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis",
    "authors": "Authors:Jun-Min Lee, Tae-Bin Ha",
    "abstract": " Abstract:  Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised learning which does not directly refer to the text of the training data to overcome the data memorization issue. By adopting this novel method, TESGAN can synthesize new sentences, showing the potential of unsupervised learning for text synthesis. We expect to see extended research combining Large Language Models with a new perspective of viewing text as an continuous space.      ",
    "date": "Submitted on 19 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17181",
    "pdf_link": "https://arxiv.org/pdf/2306.17181"
  },
  {
    "title": "Title:The Importance of Human-Labeled Data in the Era of LLMs",
    "authors": "Authors:Yang Liu",
    "abstract": "Abstract:  The advent of large language models (LLMs) has brought about a revolution inthe development of tailored machine learning models and sparked debates onredefining data requirements. The automation facilitated by the training andimplementation of LLMs has led to discussions and aspirations that human-levellabeling interventions may no longer hold the same level of importance as inthe era of supervised learning. This paper presents compelling argumentssupporting the ongoing relevance of human-labeled data in the era of LLMs.    ",
    "date": "Submitted on 18 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14910",
    "pdf_link": "https://arxiv.org/pdf/2306.14910"
  },
  {
    "title": "Title:News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking",
    "authors": "Authors:Kevin Matthe Caramancion",
    "abstract": " Abstract:  This study aimed to evaluate the proficiency of prominent Large Language Models (LLMs), namely OpenAI's ChatGPT 3.5 and 4.0, Google's Bard(LaMDA), and Microsoft's Bing AI in discerning the truthfulness of news items using black box testing. A total of 100 fact-checked news items, all sourced from independent fact-checking agencies, were presented to each of these LLMs under controlled conditions. Their responses were classified into one of three categories: True, False, and Partially True/False. The effectiveness of the LLMs was gauged based on the accuracy of their classifications against the verified facts provided by the independent agencies. The results showed a moderate proficiency across all models, with an average score of 65.25 out of 100. Among the models, OpenAI's GPT-4.0 stood out with a score of 71, suggesting an edge in newer LLMs' abilities to differentiate fact from deception. However, when juxtaposed against the performance of human fact-checkers, the AI models, despite showing promise, lag in comprehending the subtleties and contexts inherent in news information. The findings highlight the potential of AI in the domain of fact-checking while underscoring the continued importance of human cognitive skills and the necessity for persistent advancements in AI capabilities. Finally, the experimental data produced from the simulation of this work is openly available on Kaggle.      ",
    "date": "Submitted on 18 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.17176",
    "pdf_link": "https://arxiv.org/pdf/2306.17176"
  },
  {
    "title": "Title:Clickbait Classification and Spoiling Using Natural Language Processing",
    "authors": "Authors:Adhitya Thirumala, Elisa Ferracane",
    "abstract": "Abstract:  Clickbait is the practice of engineering titles to incentivize readers toclick through to articles. Such titles with sensationalized language reveal aslittle information as possible. Occasionally, clickbait will be intentionallymisleading, so natural language processing (NLP) can scan the article andanswer the question posed by the clickbait title, or spoil it. We tackle twotasks: classifying the clickbait into one of 3 types (Task 1), and spoiling theclickbait (Task 2). For Task 1, we propose two binary classifiers to determinethe final spoiler type. For Task 2, we experiment with two approaches: using aquestion-answering model to identify the span of text of the spoiler, and usinga large language model (LLM) to generate the spoiler. Because the spoiler iscontained in the article, we frame the second task as a question-answeringapproach for identifying the starting and ending positions of the spoiler. Wecreated models for Task 1 that were better than the baselines proposed by thedataset authors and engineered prompts for Task 2 that did not perform as wellas the baselines proposed by the dataset authors due to the evaluation metricperforming worse when the output text is from a generative model as opposed toan extractive model.    ",
    "date": "Submitted on 16 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14907",
    "pdf_link": "https://arxiv.org/pdf/2306.14907"
  },
  {
    "title": "Title:PRISMA-DFLLM: An Extension of PRISMA for Systematic Literature Reviews using Domain-specific Finetuned Large Language Models",
    "authors": "Authors:Teo Susnjak",
    "abstract": "Abstract:  With the proliferation of open-sourced Large Language Models (LLMs) andefficient finetuning techniques, we are on the cusp of the emergence ofnumerous domain-specific LLMs that have been finetuned for expertise acrossspecialized fields and applications for which the current general-purpose LLMsare unsuitable. In academia, this technology has the potential to revolutionizethe way we conduct systematic literature reviews (SLRs), access knowledge andgenerate new insights. This paper proposes an AI-enabled methodologicalframework that combines the power of LLMs with the rigorous reportingguidelines of the Preferred Reporting Items for Systematic Reviews andMeta-Analyses (PRISMA). By finetuning LLMs on domain-specific academic papersthat have been selected as a result of a rigorous SLR process, the proposedPRISMA-DFLLM (for Domain-specific Finetuned LLMs) reporting guidelines offerthe potential to achieve greater efficiency, reusability and scalability, whilealso opening the potential for conducting incremental living systematic reviewswith the aid of LLMs. Additionally, the proposed approach for leveraging LLMsfor SLRs enables the dissemination of finetuned models, empowering researchersto accelerate advancements and democratize cutting-edge research. This paperpresents the case for the feasibility of finetuned LLMs to support rigorousSLRs and the technical requirements for realizing this. This work then proposesthe extended PRISMA-DFLLM checklist of reporting guidelines as well as theadvantages, challenges, and potential implications of implementingPRISMA-DFLLM. Finally, a future research roadmap to develop this line ofAI-enabled SLRs is presented, paving the way for a new era of evidencesynthesis and knowledge discovery.    ",
    "date": "Submitted on 15 Jun 2023",
    "abstract_link": "https://arxiv.org/abs/2306.14905",
    "pdf_link": "https://arxiv.org/pdf/2306.14905"
  }
]